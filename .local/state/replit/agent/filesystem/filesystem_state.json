{"file_contents":{"replit.md":{"content":"# XnoxsFetcher v4.0 - TradingView Data Fetcher\n\n## Overview\nXnoxsFetcher is a Python library for fetching historical and live market data from TradingView. It supports multiple asset classes including stocks, cryptocurrencies, forex, and commodities from various exchanges worldwide.\n\n**Version:** 4.0.0  \n**Author:** developerxnoxs  \n**License:** MIT\n\n## Project Structure\n```\n.\n├── xnoxs_fetcher/              # Main library package\n│   ├── __init__.py             # Package exports\n│   ├── core.py                 # Core XnoxsFetcher class\n│   ├── live_feed.py            # Real-time data streaming\n│   ├── models.py               # Data models (SymbolSet, DataConsumer)\n│   ├── auth.py                 # NEW: Authentication manager\n│   ├── cache.py                # NEW: Local data caching\n│   ├── export.py               # NEW: CSV/Excel/JSON export\n│   ├── websocket_manager.py    # NEW: WebSocket with auto-reconnect\n│   └── parallel.py             # NEW: Parallel multi-symbol fetching\n├── demo.py                     # Basic demonstration script\n├── demo_features.py            # NEW: New features demo\n├── test_login.py               # Login test script\n├── setup.py                    # Package setup (legacy)\n├── pyproject.toml              # Modern Python packaging\n└── requirements.txt            # Python dependencies\n```\n\n## New Features in v4.0\n\n### 1. Auth Manager (`auth.py`)\nRobust session management with:\n- Session persistence to file\n- Automatic token refresh\n- Rate limiting protection\n- Retry with exponential backoff\n\n```python\nfrom xnoxs_fetcher import AuthManager\n\nauth = AuthManager()\ntoken = auth.authenticate(\"email@example.com\", \"password\")\nprint(auth.get_session_info())\n```\n\n### 2. Data Cache (`cache.py`)\nLocal SQLite caching for faster data access:\n- Automatic cache expiration\n- Cache statistics\n- Symbol/timeframe indexing\n\n```python\nfrom xnoxs_fetcher import DataCache\n\ncache = DataCache()\ncache.set(\"AAPL\", \"NASDAQ\", \"1D\", 100, dataframe)\ncached_data = cache.get(\"AAPL\", \"NASDAQ\", \"1D\", 100)\n```\n\n### 3. Data Export (`export.py`)\nExport data to multiple formats:\n- CSV export\n- Excel (.xlsx) export\n- JSON export\n- Parquet (for large datasets)\n- Summary reports\n\n```python\nfrom xnoxs_fetcher import DataExporter\n\nexporter = DataExporter()\nexporter.to_csv(data, \"AAPL_daily.csv\")\nexporter.to_excel(data, \"portfolio.xlsx\")\nexporter.to_json(data, \"data.json\")\n```\n\n### 4. WebSocket Manager (`websocket_manager.py`)\nRobust WebSocket with:\n- Auto-reconnect with exponential backoff\n- Heartbeat/ping mechanism\n- Connection state tracking\n- Event callbacks\n\n### 5. Parallel Fetcher (`parallel.py`)\nConcurrent multi-symbol fetching:\n- Thread pool execution\n- Progress callbacks\n- Cache integration\n- Error handling & retry\n\n```python\nfrom xnoxs_fetcher import fetch_parallel, TimeFrame\n\nsymbols = [(\"AAPL\", \"NASDAQ\"), (\"GOOGL\", \"NASDAQ\"), (\"MSFT\", \"NASDAQ\")]\nresults = fetch_parallel(fetcher, symbols, TimeFrame.DAILY, bars=100)\n```\n\n## Running the Project\n\n### Run Features Demo (New Features)\n```bash\npython demo_features.py\n```\n\n### Run Basic Demo\n```bash\npython demo.py\n```\n\n### Test Login\n```bash\npython test_login.py\n```\n\n## Basic Usage\n\n```python\nfrom xnoxs_fetcher import XnoxsFetcher, TimeFrame\n\n# Initialize fetcher (anonymous mode)\nfetcher = XnoxsFetcher()\n\n# Fetch historical data\ndata = fetcher.get_historical_data(\n    symbol=\"AAPL\",\n    exchange=\"NASDAQ\",\n    timeframe=TimeFrame.DAILY,\n    bars=100\n)\n\nprint(data)\n```\n\n### Using with Authentication\n```python\nfetcher = XnoxsFetcher(\n    username=\"your_tradingview_email@example.com\",\n    password=\"your_password\"\n)\n```\n\n### Live Data Streaming\n```python\nfrom xnoxs_fetcher import XnoxsLiveFeed, TimeFrame\n\nlive = XnoxsLiveFeed()\nseis = live.create_symbol_set(\"BTCUSD\", \"BINANCE\", TimeFrame.MINUTE_1)\n\ndef on_update(symbol_set, data):\n    print(f\"BTC: ${data['close'].iloc[0]:,.2f}\")\n\nconsumer = seis.create_consumer(on_update)\n```\n\n## Key Features\n- **Multiple Asset Classes:** Stocks, crypto, forex, commodities\n- **Global Exchanges:** NASDAQ, NYSE, Binance, IDX, NSE, BSE, and more\n- **Flexible Timeframes:** From 1-minute to monthly charts\n- **Real-time Streaming:** Live data with callback-based consumers\n- **Pandas Integration:** Returns data as Pandas DataFrames\n- **Session Persistence:** Save and restore login sessions\n- **Data Caching:** Local SQLite cache for faster access\n- **Multi-format Export:** CSV, Excel, JSON, Parquet\n- **Parallel Fetching:** Concurrent multi-symbol data retrieval\n- **Auto-reconnect:** Robust WebSocket connection management\n\n## Supported Timeframes\n- Minutes: 1, 3, 5, 15, 30, 45\n- Hours: 1, 2, 3, 4\n- Days+: Daily, Weekly, Monthly\n\n## Dependencies\n- pandas >= 2.0.0\n- websocket-client >= 1.0.0\n- requests >= 2.25.0\n- python-dateutil >= 2.8.0\n- openpyxl >= 3.1.0 (Excel export)\n- pyarrow >= 14.0.0 (Parquet export)\n\n## Environment Variables\n- `TRADINGVIEW_USERNAME` - TradingView email\n- `TRADINGVIEW_PASSWORD` - TradingView password\n\n## Recent Changes\n- December 2, 2025: v4.0 Major Update\n  - Added AuthManager for robust session management\n  - Added DataCache for local data caching\n  - Added DataExporter for multi-format export\n  - Added WebSocketManager with auto-reconnect\n  - Added ParallelFetcher for concurrent requests\n  - Fixed login headers to match browser requests\n  - Updated all dependencies\n","path":null,"size_bytes":5461,"size_tokens":null},"xnoxs_fetcher/live_feed.py":{"content":"\"\"\"\nXnoxsFetcher Live Feed Module\n\nThis module provides real-time data streaming capabilities\nwith callback-based consumer pattern.\n\nAuthor: developerxnoxs\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport threading\nimport time\nfrom datetime import datetime\nfrom typing import Callable, Dict, List, Optional, Any, Tuple\n\nimport pandas as pd\nfrom dateutil.relativedelta import relativedelta\n\nfrom .core import XnoxsFetcher, TimeFrame\nfrom .models import SymbolSet, DataConsumer\n\nlogger = logging.getLogger(__name__)\n\nRETRY_LIMIT = 50\n\n\nclass IntervalTracker(dict):\n    \"\"\"\n    Internal class for managing interval groups and trigger times.\n    \n    Tracks multiple SymbolSets organized by their intervals,\n    and manages waiting for the next data update.\n    \"\"\"\n    \n    _TIMEFRAME_DELTAS = {\n        \"1\": relativedelta(minutes=1),\n        \"3\": relativedelta(minutes=3),\n        \"5\": relativedelta(minutes=5),\n        \"15\": relativedelta(minutes=15),\n        \"30\": relativedelta(minutes=30),\n        \"45\": relativedelta(minutes=45),\n        \"1H\": relativedelta(hours=1),\n        \"2H\": relativedelta(hours=2),\n        \"3H\": relativedelta(hours=3),\n        \"4H\": relativedelta(hours=4),\n        \"1D\": relativedelta(days=1),\n        \"1W\": relativedelta(weeks=1),\n        \"1M\": relativedelta(months=1),\n    }\n    \n    def __init__(self) -> None:\n        super().__init__()\n        self._shutdown_flag = False\n        self._next_trigger: Optional[datetime] = None\n        self._interrupt_event = threading.Event()\n    \n    def _calculate_next_trigger(self) -> Optional[datetime]:\n        \"\"\"Get the soonest expiry datetime across all intervals.\"\"\"\n        if not self.values():\n            return None\n        \n        trigger_times = [values[1] for values in self.values()]\n        trigger_times.sort()\n        return trigger_times[0]\n    \n    def find_symbol_set(\n        self, \n        symbol: str, \n        exchange: str, \n        interval: TimeFrame\n    ) -> Optional[SymbolSet]:\n        \"\"\"Find existing SymbolSet by parameters.\"\"\"\n        for seis in self:\n            if (\n                seis.symbol == symbol \n                and seis.exchange == exchange \n                and seis.interval == interval\n            ):\n                return seis\n        return None\n    \n    def wait_for_trigger(self) -> bool:\n        \"\"\"\n        Wait until next interval expires.\n        \n        Returns:\n            True if wait completed normally\n            False if shutdown requested\n        \"\"\"\n        if not self._shutdown_flag:\n            self._interrupt_event.clear()\n        \n        self._next_trigger = self._calculate_next_trigger()\n        \n        while True:\n            wait_duration = (self._next_trigger - datetime.now()).total_seconds()\n            \n            interrupted = self._interrupt_event.wait(wait_duration)\n            \n            if interrupted and self._shutdown_flag:\n                return False\n            \n            if not interrupted:\n                self._interrupt_event.clear()\n                break\n        \n        return True\n    \n    def get_expired_intervals(self) -> List[str]:\n        \"\"\"Get intervals that have expired and update their next trigger times.\"\"\"\n        expired = []\n        now = datetime.now()\n        \n        for interval_key, values in self.items():\n            if now >= values[1]:\n                expired.append(interval_key)\n                values[1] = values[1] + self._TIMEFRAME_DELTAS[interval_key]\n        \n        return expired\n    \n    def request_shutdown(self) -> None:\n        \"\"\"Signal shutdown and interrupt waiting.\"\"\"\n        self._shutdown_flag = True\n        self._interrupt_event.set()\n    \n    def add_symbol_set(\n        self, \n        seis: SymbolSet,\n        update_time: Optional[datetime] = None\n    ) -> None:\n        \"\"\"Add SymbolSet to tracking.\"\"\"\n        if self:\n            self._shutdown_flag = False\n            self._interrupt_event.clear()\n        \n        interval_key = seis.interval.value\n        \n        if interval_key in self.keys():\n            super().__getitem__(interval_key)[0].append(seis)\n        else:\n            if update_time is None:\n                raise ValueError(\"update_time required for new interval group\")\n            \n            next_trigger = update_time + self._TIMEFRAME_DELTAS[interval_key]\n            self[interval_key] = [[seis], next_trigger]\n            \n            calculated_trigger = self._calculate_next_trigger()\n            if calculated_trigger != self._next_trigger:\n                self._next_trigger = calculated_trigger\n                self._interrupt_event.set()\n    \n    def remove_symbol_set(self, seis: SymbolSet) -> None:\n        \"\"\"Remove SymbolSet from tracking.\"\"\"\n        if seis not in self:\n            raise KeyError(\"SymbolSet not found in tracker\")\n        \n        interval_key = seis.interval.value\n        super().__getitem__(interval_key)[0].remove(seis)\n        \n        if not super().__getitem__(interval_key)[0]:\n            self.pop(interval_key)\n            \n            calculated_trigger = self._calculate_next_trigger()\n            if calculated_trigger != self._next_trigger and not self._shutdown_flag:\n                self._next_trigger = calculated_trigger\n                self._interrupt_event.set()\n    \n    def get_intervals(self) -> List[str]:\n        \"\"\"Get list of tracked interval keys.\"\"\"\n        return list(self.keys())\n    \n    def __getitem__(self, interval_key: str) -> List[SymbolSet]:\n        \"\"\"Get SymbolSets for an interval.\"\"\"\n        return super().__getitem__(interval_key)[0]\n    \n    def __iter__(self):\n        \"\"\"Iterate over all SymbolSets.\"\"\"\n        all_sets = []\n        for seis_list in super().values():\n            all_sets.extend(seis_list[0])\n        return iter(all_sets)\n    \n    def __contains__(self, seis: object) -> bool:\n        \"\"\"Check if SymbolSet is tracked.\"\"\"\n        for seis_list in super().values():\n            if seis in seis_list[0]:\n                return True\n        return False\n\n\nclass XnoxsLiveFeed(XnoxsFetcher):\n    \"\"\"\n    Real-time TradingView Data Feed.\n    \n    Extends XnoxsFetcher with live data streaming capabilities.\n    Supports multiple symbol-exchange-interval sets with \n    callback-based data consumption.\n    \n    Features:\n        - Automatic interval-based data fetching\n        - Multiple consumers per symbol set\n        - Thread-safe operations\n        - Graceful shutdown handling\n        \n    Example:\n        >>> live = XnoxsLiveFeed()\n        >>> seis = live.create_symbol_set(\"BTCUSD\", \"BINANCE\", TimeFrame.MINUTE_1)\n        >>> \n        >>> def on_price(seis, data):\n        ...     print(f\"BTC: ${data['close'].iloc[0]:.2f}\")\n        >>> \n        >>> consumer = seis.create_consumer(on_price)\n        \n    Author: developerxnoxs\n    \"\"\"\n    \n    __slots__ = (\"_lock\", \"_main_thread\", \"_tracker\")\n    \n    def __init__(\n        self, \n        username: Optional[str] = None,\n        password: Optional[str] = None\n    ) -> None:\n        \"\"\"\n        Initialize XnoxsLiveFeed.\n        \n        Args:\n            username: TradingView username (optional)\n            password: TradingView password (optional)\n        \"\"\"\n        super().__init__(username, password)\n        self._lock = threading.Lock()\n        self._main_thread: Optional[threading.Thread] = None\n        self._tracker = IntervalTracker()\n    \n    def _validate_symbol(self, symbol: str, exchange: str) -> bool:\n        \"\"\"Check if symbol exists on TradingView.\"\"\"\n        results = self.search_symbols(symbol, exchange)\n        \n        if not results:\n            return False\n        \n        for item in results:\n            if item.get(\"symbol\") == symbol and item.get(\"exchange\") == exchange:\n                return True\n        \n        return False\n    \n    def create_symbol_set(\n        self,\n        symbol: str,\n        exchange: str,\n        interval: TimeFrame,\n        timeout: float = -1\n    ) -> SymbolSet:\n        \"\"\"\n        Create and register a new SymbolSet for live tracking.\n        \n        Args:\n            symbol: Trading symbol\n            exchange: Exchange name  \n            interval: Chart timeframe\n            timeout: Maximum wait time in seconds (-1 for blocking)\n            \n        Returns:\n            Created or existing SymbolSet\n            \n        Raises:\n            ValueError: If symbol/exchange combination not found\n        \"\"\"\n        existing = self._tracker.find_symbol_set(symbol, exchange, interval)\n        if existing:\n            return existing\n        \n        new_seis = SymbolSet(symbol, exchange, interval)\n        \n        acquired = self._lock.acquire(timeout=timeout if timeout > 0 else -1)\n        if not acquired:\n            raise TimeoutError(\"Lock acquisition timed out\")\n        \n        try:\n            new_seis.live_feed = self\n            \n            if new_seis in self._tracker:\n                return self._tracker.find_symbol_set(symbol, exchange, interval)\n            \n            interval_key = new_seis.interval.value\n            \n            if interval_key not in self._tracker.get_intervals():\n                ticker_data = super().get_historical_data(\n                    new_seis.symbol,\n                    new_seis.exchange,\n                    new_seis.interval,\n                    bars=2\n                )\n                update_time = ticker_data.index.to_pydatetime()[0]\n                self._tracker.add_symbol_set(new_seis, update_time)\n            else:\n                self._tracker.add_symbol_set(new_seis)\n        finally:\n            self._lock.release()\n        \n        if self._main_thread is None:\n            self._main_thread = threading.Thread(\n                name=\"xnoxs_live_feed\",\n                target=self._data_loop,\n                daemon=True\n            )\n            self._main_thread.start()\n        \n        return new_seis\n    \n    def remove_symbol_set(\n        self, \n        seis: SymbolSet,\n        timeout: float = -1\n    ) -> bool:\n        \"\"\"\n        Remove SymbolSet from live tracking.\n        \n        Args:\n            seis: SymbolSet to remove\n            timeout: Maximum wait time in seconds\n            \n        Returns:\n            True if successful, False if timed out\n        \"\"\"\n        if seis not in self._tracker:\n            raise ValueError(\"SymbolSet not registered\")\n        \n        acquired = self._lock.acquire(timeout=timeout if timeout > 0 else -1)\n        if not acquired:\n            return False\n        \n        try:\n            for consumer in seis.get_consumers():\n                consumer.enqueue(None)\n            \n            self._tracker.remove_symbol_set(seis)\n            del seis.live_feed\n            \n            if not self._tracker:\n                self._tracker.request_shutdown()\n        finally:\n            self._lock.release()\n        \n        return True\n    \n    def create_consumer(\n        self,\n        seis: SymbolSet,\n        callback: Callable[[SymbolSet, pd.DataFrame], None],\n        timeout: float = -1\n    ) -> DataConsumer:\n        \"\"\"\n        Create a new data consumer for SymbolSet.\n        \n        Args:\n            seis: SymbolSet to consume data from\n            callback: Function to call on new data\n            timeout: Maximum wait time in seconds\n            \n        Returns:\n            Created DataConsumer instance\n        \"\"\"\n        if seis not in self._tracker:\n            raise ValueError(\"SymbolSet not registered\")\n        \n        consumer = DataConsumer(seis, callback)\n        \n        acquired = self._lock.acquire(timeout=timeout if timeout > 0 else -1)\n        if not acquired:\n            raise TimeoutError(\"Lock acquisition timed out\")\n        \n        try:\n            seis.register_consumer(consumer)\n            consumer.start()\n        finally:\n            self._lock.release()\n        \n        return consumer\n    \n    def remove_consumer(\n        self, \n        consumer: DataConsumer,\n        timeout: float = -1\n    ) -> bool:\n        \"\"\"\n        Remove consumer from its SymbolSet.\n        \n        Args:\n            consumer: Consumer to remove\n            timeout: Maximum wait time in seconds\n            \n        Returns:\n            True if successful, False if timed out\n        \"\"\"\n        acquired = self._lock.acquire(timeout=timeout if timeout > 0 else -1)\n        if not acquired:\n            return False\n        \n        try:\n            consumer.symbol_set.unregister_consumer(consumer)\n            consumer.stop()\n        finally:\n            self._lock.release()\n        \n        return True\n    \n    def _data_loop(self) -> None:\n        \"\"\"Main data fetching loop (runs in background thread).\"\"\"\n        while self._tracker.wait_for_trigger():\n            with self._lock:\n                for interval_key in self._tracker.get_expired_intervals():\n                    for seis in self._tracker[interval_key]:\n                        data: Optional[pd.DataFrame] = None\n                        \n                        for attempt in range(RETRY_LIMIT):\n                            data = super().get_historical_data(\n                                seis.symbol,\n                                seis.exchange,\n                                timeframe=seis.interval,\n                                bars=2\n                            )\n                            \n                            if data is not None and seis.is_new_data(data):\n                                data = data.drop(labels=data.index[1])\n                                break\n                            \n                            time.sleep(0.1)\n                        else:\n                            self._tracker.request_shutdown()\n                            logger.critical(\n                                \"Failed to fetch data from TradingView\"\n                            )\n                            continue\n                        \n                        for consumer in seis.get_consumers():\n                            consumer.enqueue(data)\n        \n        with self._lock:\n            for seis in list(self._tracker):\n                for consumer in seis.get_consumers():\n                    seis.unregister_consumer(consumer)\n                    consumer.stop()\n                \n                self._tracker.remove_symbol_set(seis)\n            \n            self._main_thread = None\n    \n    def get_historical_data(\n        self,\n        symbol: str,\n        exchange: str = \"NSE\",\n        timeframe: TimeFrame = TimeFrame.DAILY,\n        bars: int = 10,\n        futures_contract: Optional[int] = None,\n        extended_session: bool = False,\n        timeout: float = -1,\n    ) -> Optional[pd.DataFrame]:\n        \"\"\"\n        Get historical data (thread-safe version).\n        \n        Overrides parent method to add thread safety.\n        See XnoxsFetcher.get_historical_data for full documentation.\n        \"\"\"\n        acquired = self._lock.acquire(timeout=timeout if timeout > 0 else -1)\n        if not acquired:\n            return None\n        \n        try:\n            return super().get_historical_data(\n                symbol, exchange, timeframe,\n                bars, futures_contract, extended_session\n            )\n        finally:\n            self._lock.release()\n    \n    def shutdown(self) -> None:\n        \"\"\"Stop all consumers and close live feed.\"\"\"\n        if self._main_thread is not None:\n            with self._lock:\n                self._tracker.request_shutdown()\n            self._main_thread.join()\n    \n    def __del__(self) -> None:\n        \"\"\"Cleanup on deletion.\"\"\"\n        self.shutdown()\n\n\n# Backward compatibility alias\nTvDatafeedLive = XnoxsLiveFeed\n","path":null,"size_bytes":15682,"size_tokens":null},"test_login.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nXnoxsFetcher Login Test Script\n\nTests the TradingView authentication feature.\n\"\"\"\n\nimport os\nimport sys\n\nfrom xnoxs_fetcher import XnoxsFetcher, TimeFrame\n\n\ndef print_header(title: str) -> None:\n    \"\"\"Print formatted section header.\"\"\"\n    line = \"=\" * 60\n    print(f\"\\n{line}\")\n    print(f\"  {title}\")\n    print(f\"{line}\\n\")\n\n\ndef test_login() -> int:\n    \"\"\"Test login functionality.\"\"\"\n    print_header(\"XnoxsFetcher Login Test\")\n    \n    username = os.environ.get(\"TRADINGVIEW_USERNAME\")\n    password = os.environ.get(\"TRADINGVIEW_PASSWORD\")\n    \n    if not username or not password:\n        print(\"  [ERROR] Kredensial tidak ditemukan!\")\n        print()\n        print(\"  Silakan set environment variables:\")\n        print(\"    - TRADINGVIEW_USERNAME (email TradingView Anda)\")\n        print(\"    - TRADINGVIEW_PASSWORD (password TradingView Anda)\")\n        print()\n        return 1\n    \n    print(f\"  Username: {username}\")\n    print(f\"  Password: {'*' * len(password)}\")\n    print()\n    print(\"  Mencoba login ke TradingView...\")\n    \n    try:\n        fetcher = XnoxsFetcher(username=username, password=password)\n        \n        if fetcher.token == \"unauthorized_user_token\":\n            print(\"  [GAGAL] Login tidak berhasil!\")\n            print(\"  Kemungkinan penyebab:\")\n            print(\"    - Username atau password salah\")\n            print(\"    - Akun TradingView membutuhkan verifikasi 2FA\")\n            print(\"    - TradingView memblokir login dari server\")\n            return 1\n        \n        print(\"  [OK] Login berhasil!\")\n        token_preview = fetcher.token[:30] if len(fetcher.token) > 30 else fetcher.token\n        print(f\"  Token: {token_preview}...\")\n        print()\n        \n        print(\"  Mengambil data dengan akun terautentikasi...\")\n        data = fetcher.get_historical_data(\n            symbol=\"AAPL\",\n            exchange=\"NASDAQ\",\n            timeframe=TimeFrame.DAILY,\n            bars=5\n        )\n        \n        if data is not None and not data.empty:\n            print(\"  [OK] Data berhasil diambil!\")\n            print()\n            print(data.to_string())\n        else:\n            print(\"  [GAGAL] Tidak bisa mengambil data\")\n            \n    except Exception as exc:\n        print(f\"  [ERROR] {exc}\")\n        return 1\n    \n    print_header(\"Test Selesai\")\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(test_login())\n","path":null,"size_bytes":2402,"size_tokens":null},"pyproject.toml":{"content":"[build-system]\nrequires = [\"setuptools>=61.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"xnoxs_fetcher\"\nversion = \"3.0.0\"\ndescription = \"Advanced TradingView historical and live data fetcher\"\nreadme = \"README.md\"\nlicense = {text = \"MIT\"}\nauthors = [\n    {name = \"developerxnoxs\", email = \"developerxnoxs@gmail.com\"}\n]\nkeywords = [\n    \"tradingview\",\n    \"trading\",\n    \"stocks\",\n    \"crypto\",\n    \"cryptocurrency\",\n    \"forex\",\n    \"market-data\",\n    \"historical-data\",\n    \"live-data\",\n    \"ohlcv\",\n    \"pandas\"\n]\nclassifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"Intended Audience :: Developers\",\n    \"Intended Audience :: Financial and Insurance Industry\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Topic :: Office/Business :: Financial :: Investment\",\n    \"Topic :: Software Development :: Libraries :: Python Modules\",\n    \"Typing :: Typed\"\n]\nrequires-python = \">=3.9\"\ndependencies = [\n    \"pandas>=2.0.0\",\n    \"websocket-client>=1.0.0\",\n    \"requests>=2.25.0\",\n    \"python-dateutil>=2.8.0\"\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest>=7.0.0\",\n    \"pytest-cov>=4.0.0\",\n    \"black>=23.0.0\",\n    \"mypy>=1.0.0\",\n    \"ruff>=0.1.0\"\n]\n\n[project.urls]\nHomepage = \"https://github.com/developerxnoxs/xnoxs_fetcher\"\nDocumentation = \"https://github.com/developerxnoxs/xnoxs_fetcher#readme\"\nRepository = \"https://github.com/developerxnoxs/xnoxs_fetcher\"\n\"Bug Tracker\" = \"https://github.com/developerxnoxs/xnoxs_fetcher/issues\"\n\n[tool.setuptools.packages.find]\nexclude = [\"tests\", \"tests.*\"]\n","path":null,"size_bytes":1807,"size_tokens":null},"xnoxs_fetcher/__init__.py":{"content":"\"\"\"\nXnoxsFetcher - Advanced TradingView Data Fetcher\n\nA powerful Python library for fetching historical and live market data \nfrom TradingView. Supports multiple exchanges, timeframes, and real-time\ndata streaming with callback-based consumers.\n\nAuthor: developerxnoxs\nLicense: MIT License\nVersion: 4.0.0\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom .core import XnoxsFetcher, TimeFrame, FetcherConfig\nfrom .live_feed import XnoxsLiveFeed\nfrom .models import SymbolSet, DataConsumer\nfrom .auth import AuthManager, AuthConfig, SessionData, RateLimiter\nfrom .cache import DataCache, CacheConfig, CacheTTL\nfrom .export import DataExporter, quick_export\nfrom .websocket_manager import WebSocketManager, WebSocketConfig, ConnectionState, WebSocketPool\nfrom .parallel import ParallelFetcher, ParallelConfig, FetchTask, FetchResult, fetch_parallel, BatchExporter\n\n__version__ = \"4.0.0\"\n__author__ = \"developerxnoxs\"\n\nTvDatafeed = XnoxsFetcher\nInterval = TimeFrame\nTvDatafeedLive = XnoxsLiveFeed\nSeis = SymbolSet\nConsumer = DataConsumer\n\n__all__ = [\n    \"XnoxsFetcher\",\n    \"XnoxsLiveFeed\", \n    \"TimeFrame\",\n    \"SymbolSet\",\n    \"DataConsumer\",\n    \"FetcherConfig\",\n    \"TvDatafeed\",\n    \"Interval\",\n    \"TvDatafeedLive\",\n    \"Seis\",\n    \"Consumer\",\n    \"AuthManager\",\n    \"AuthConfig\",\n    \"SessionData\",\n    \"RateLimiter\",\n    \"DataCache\",\n    \"CacheConfig\",\n    \"CacheTTL\",\n    \"DataExporter\",\n    \"quick_export\",\n    \"WebSocketManager\",\n    \"WebSocketConfig\",\n    \"ConnectionState\",\n    \"WebSocketPool\",\n    \"ParallelFetcher\",\n    \"ParallelConfig\",\n    \"FetchTask\",\n    \"FetchResult\",\n    \"fetch_parallel\",\n    \"BatchExporter\",\n]\n","path":null,"size_bytes":1628,"size_tokens":null},"xnoxs_fetcher/models.py":{"content":"\"\"\"\nXnoxsFetcher Data Models\n\nThis module contains data models for symbol sets and data consumers\nused in live data streaming.\n\nAuthor: developerxnoxs\n\"\"\"\n\nfrom __future__ import annotations\n\nimport queue\nimport threading\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Callable, List, Optional, Any\n\nimport pandas as pd\n\nif TYPE_CHECKING:\n    from .live_feed import XnoxsLiveFeed\n    from .core import TimeFrame\n\n\nclass SymbolSet:\n    \"\"\"\n    Symbol-Exchange-Interval Set (SEIS) Container.\n    \n    Represents a unique combination of trading symbol, exchange,\n    and chart interval. Manages consumers registered for this\n    particular data stream.\n    \n    Attributes:\n        symbol: Trading symbol (e.g., \"AAPL\", \"BTCUSD\")\n        exchange: Exchange name (e.g., \"NASDAQ\", \"BINANCE\")\n        interval: Chart timeframe interval\n        \n    Example:\n        >>> seis = SymbolSet(\"AAPL\", \"NASDAQ\", TimeFrame.HOUR_1)\n        >>> print(seis)\n        symbol='AAPL',exchange='NASDAQ',interval='HOUR_1'\n        \n    Author: developerxnoxs\n    \"\"\"\n    \n    __slots__ = (\n        \"_symbol\", \"_exchange\", \"_interval\", \n        \"_live_feed\", \"_consumers\", \"_last_update\"\n    )\n    \n    def __init__(\n        self, \n        symbol: str, \n        exchange: str, \n        interval: \"TimeFrame\"\n    ) -> None:\n        \"\"\"\n        Initialize SymbolSet.\n        \n        Args:\n            symbol: Trading symbol\n            exchange: Exchange name\n            interval: Chart timeframe\n        \"\"\"\n        self._symbol = symbol\n        self._exchange = exchange\n        self._interval = interval\n        self._live_feed: Optional[\"XnoxsLiveFeed\"] = None\n        self._consumers: List[\"DataConsumer\"] = []\n        self._last_update: Optional[Any] = None\n    \n    def __eq__(self, other: object) -> bool:\n        \"\"\"Check equality based on symbol, exchange, and interval.\"\"\"\n        if not isinstance(other, SymbolSet):\n            return NotImplemented\n        return (\n            self._symbol == other._symbol \n            and self._exchange == other._exchange \n            and self._interval == other._interval\n        )\n    \n    def __repr__(self) -> str:\n        \"\"\"Return machine-readable representation.\"\"\"\n        return f'SymbolSet(\"{self._symbol}\",\"{self._exchange}\",{self._interval})'\n    \n    def __str__(self) -> str:\n        \"\"\"Return human-readable representation.\"\"\"\n        return (\n            f\"symbol='{self._symbol}',\"\n            f\"exchange='{self._exchange}',\"\n            f\"interval='{self._interval.name}'\"\n        )\n    \n    def __hash__(self) -> int:\n        \"\"\"Make SymbolSet hashable for use in sets/dicts.\"\"\"\n        return hash((self._symbol, self._exchange, self._interval))\n    \n    @property\n    def symbol(self) -> str:\n        \"\"\"Get trading symbol.\"\"\"\n        return self._symbol\n    \n    @property\n    def exchange(self) -> str:\n        \"\"\"Get exchange name.\"\"\"\n        return self._exchange\n    \n    @property\n    def interval(self) -> \"TimeFrame\":\n        \"\"\"Get chart interval.\"\"\"\n        return self._interval\n    \n    @property\n    def live_feed(self) -> Optional[\"XnoxsLiveFeed\"]:\n        \"\"\"Get associated live feed instance.\"\"\"\n        return self._live_feed\n    \n    @live_feed.setter\n    def live_feed(self, value: \"XnoxsLiveFeed\") -> None:\n        \"\"\"Set live feed instance.\"\"\"\n        if self._live_feed is not None:\n            raise AttributeError(\n                \"Cannot overwrite live_feed - delete first\"\n            )\n        self._live_feed = value\n    \n    @live_feed.deleter\n    def live_feed(self) -> None:\n        \"\"\"Remove live feed reference.\"\"\"\n        self._live_feed = None\n    \n    def create_consumer(\n        self, \n        callback: Callable[[\"SymbolSet\", pd.DataFrame], None],\n        timeout: float = -1\n    ) -> \"DataConsumer\":\n        \"\"\"\n        Create and register a new data consumer.\n        \n        Args:\n            callback: Function to call when new data arrives\n                     Signature: callback(symbol_set, dataframe)\n            timeout: Maximum wait time in seconds (-1 for blocking)\n            \n        Returns:\n            Created DataConsumer instance\n            \n        Raises:\n            RuntimeError: If no live feed is associated\n        \"\"\"\n        if self._live_feed is None:\n            raise RuntimeError(\"No live feed associated with this SymbolSet\")\n        return self._live_feed.create_consumer(self, callback, timeout)\n    \n    def remove_consumer(\n        self, \n        consumer: \"DataConsumer\",\n        timeout: float = -1\n    ) -> bool:\n        \"\"\"\n        Remove a consumer from this SymbolSet.\n        \n        Args:\n            consumer: Consumer to remove\n            timeout: Maximum wait time in seconds\n            \n        Returns:\n            True if successful, False if timed out\n        \"\"\"\n        if self._live_feed is None:\n            raise RuntimeError(\"No live feed associated\")\n        return self._live_feed.remove_consumer(consumer, timeout)\n    \n    def register_consumer(self, consumer: \"DataConsumer\") -> None:\n        \"\"\"Internal: Register consumer in list.\"\"\"\n        self._consumers.append(consumer)\n    \n    def unregister_consumer(self, consumer: \"DataConsumer\") -> None:\n        \"\"\"Internal: Remove consumer from list.\"\"\"\n        if consumer not in self._consumers:\n            raise ValueError(\"Consumer not found in SymbolSet\")\n        self._consumers.remove(consumer)\n    \n    def is_new_data(self, data: pd.DataFrame) -> bool:\n        \"\"\"\n        Check if data is newer than last received.\n        \n        Args:\n            data: DataFrame with datetime index\n            \n        Returns:\n            True if data is new, False otherwise\n        \"\"\"\n        current_time = data.index.to_pydatetime()[0]\n        if self._last_update != current_time:\n            self._last_update = current_time\n            return True\n        return False\n    \n    def get_historical_data(\n        self, \n        bars: int = 10,\n        timeout: float = -1\n    ) -> Optional[pd.DataFrame]:\n        \"\"\"\n        Get historical data for this SymbolSet.\n        \n        Args:\n            bars: Number of bars to retrieve\n            timeout: Maximum wait time in seconds\n            \n        Returns:\n            DataFrame with OHLCV data\n            \n        Raises:\n            RuntimeError: If no live feed is associated\n        \"\"\"\n        if self._live_feed is None:\n            raise RuntimeError(\"No live feed associated\")\n        return self._live_feed.get_historical_data(\n            symbol=self._symbol,\n            exchange=self._exchange,\n            timeframe=self._interval,\n            bars=bars,\n            timeout=timeout\n        )\n    \n    def remove(self, timeout: float = -1) -> bool:\n        \"\"\"\n        Remove this SymbolSet from live feed.\n        \n        Args:\n            timeout: Maximum wait time in seconds\n            \n        Returns:\n            True if successful, False if timed out\n        \"\"\"\n        if self._live_feed is None:\n            raise RuntimeError(\"No live feed associated\")\n        return self._live_feed.remove_symbol_set(self, timeout)\n    \n    def get_consumers(self) -> List[\"DataConsumer\"]:\n        \"\"\"Get list of registered consumers.\"\"\"\n        return self._consumers.copy()\n\n\nclass DataConsumer(threading.Thread):\n    \"\"\"\n    Asynchronous Data Consumer.\n    \n    Runs in a separate thread, processing incoming market data\n    and invoking callback functions for each new data bar.\n    \n    Attributes:\n        symbol_set: Associated SymbolSet\n        callback: Function to invoke on new data\n        \n    Example:\n        >>> def on_data(seis, data):\n        ...     print(f\"New data: {data['close'].iloc[0]}\")\n        >>> consumer = live_feed.create_consumer(seis, on_data)\n        \n    Author: developerxnoxs\n    \"\"\"\n    \n    __slots__ = (\"_buffer\", \"symbol_set\", \"callback\")\n    \n    def __init__(\n        self, \n        symbol_set: SymbolSet,\n        callback: Callable[[SymbolSet, pd.DataFrame], None]\n    ) -> None:\n        \"\"\"\n        Initialize DataConsumer.\n        \n        Args:\n            symbol_set: SymbolSet to consume data from\n            callback: Function to call with new data\n        \"\"\"\n        super().__init__()\n        self._buffer: queue.Queue[Optional[pd.DataFrame]] = queue.Queue()\n        self.symbol_set = symbol_set\n        self.callback = callback\n        \n        self.name = (\n            f\"{callback.__name__}_\"\n            f\"{symbol_set.symbol}_\"\n            f\"{symbol_set.exchange}_\"\n            f\"{symbol_set.interval.value}\"\n        )\n    \n    def __repr__(self) -> str:\n        \"\"\"Return machine-readable representation.\"\"\"\n        return f\"DataConsumer({repr(self.symbol_set)},{self.callback.__name__})\"\n    \n    def __str__(self) -> str:\n        \"\"\"Return human-readable representation.\"\"\"\n        return f\"{repr(self.symbol_set)},callback={self.callback.__name__}\"\n    \n    def run(self) -> None:\n        \"\"\"Thread main loop - process data queue.\"\"\"\n        while True:\n            data = self._buffer.get()\n            \n            if data is None:\n                break\n            \n            try:\n                self.callback(self.symbol_set, data)\n            except Exception as exc:\n                self.remove()\n                self.symbol_set = None  # type: ignore\n                self.callback = None  # type: ignore\n                self._buffer = None  # type: ignore\n                raise exc from None\n        \n        self.symbol_set = None  # type: ignore\n        self.callback = None  # type: ignore\n        self._buffer = None  # type: ignore\n    \n    def enqueue(self, data: Optional[pd.DataFrame]) -> None:\n        \"\"\"\n        Add data to processing queue.\n        \n        Args:\n            data: DataFrame to process, or None to stop\n        \"\"\"\n        self._buffer.put(data)\n    \n    def remove(self, timeout: float = -1) -> bool:\n        \"\"\"\n        Stop consumer and remove from SymbolSet.\n        \n        Args:\n            timeout: Maximum wait time in seconds\n            \n        Returns:\n            True if successful, False if timed out\n        \"\"\"\n        return self.symbol_set.remove_consumer(self, timeout)\n    \n    def stop(self) -> None:\n        \"\"\"Signal thread to stop processing.\"\"\"\n        self._buffer.put(None)\n\n\n# Backward compatibility aliases\nSeis = SymbolSet\nConsumer = DataConsumer\n","path":null,"size_bytes":10385,"size_tokens":null},"setup.py":{"content":"\"\"\"\nXnoxsFetcher Package Setup\n\nAdvanced TradingView Data Fetcher - A powerful Python library\nfor downloading historical and streaming live market data.\n\nAuthor: developerxnoxs\nLicense: MIT\n\"\"\"\n\nfrom setuptools import setup, find_packages\nfrom pathlib import Path\n\nREADME_PATH = Path(__file__).parent / \"README.md\"\nlong_description = README_PATH.read_text(encoding=\"utf-8\") if README_PATH.exists() else \"\"\n\nsetup(\n    name=\"xnoxs_fetcher\",\n    version=\"3.0.0\",\n    author=\"developerxnoxs\",\n    author_email=\"developerxnoxs@gmail.com\",\n    description=\"Advanced TradingView historical and live data fetcher\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/developerxnoxs/xnoxs_fetcher\",\n    project_urls={\n        \"Bug Tracker\": \"https://github.com/developerxnoxs/xnoxs_fetcher/issues\",\n        \"Source\": \"https://github.com/developerxnoxs/xnoxs_fetcher\",\n        \"Documentation\": \"https://github.com/developerxnoxs/xnoxs_fetcher#readme\",\n    },\n    license=\"MIT\",\n    packages=find_packages(exclude=[\"tests\", \"tests.*\", \"tvDatafeed\"]),\n    python_requires=\">=3.9\",\n    install_requires=[\n        \"pandas>=2.0.0\",\n        \"websocket-client>=1.0.0\",\n        \"requests>=2.25.0\",\n        \"python-dateutil>=2.8.0\",\n    ],\n    extras_require={\n        \"dev\": [\n            \"pytest>=7.0.0\",\n            \"pytest-cov>=4.0.0\",\n            \"black>=23.0.0\",\n            \"mypy>=1.0.0\",\n            \"ruff>=0.1.0\",\n        ],\n    },\n    classifiers=[\n        \"Development Status :: 4 - Beta\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Financial and Insurance Industry\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Operating System :: OS Independent\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"Topic :: Office/Business :: Financial :: Investment\",\n        \"Topic :: Software Development :: Libraries :: Python Modules\",\n        \"Typing :: Typed\",\n    ],\n    keywords=[\n        \"tradingview\",\n        \"trading\",\n        \"stocks\",\n        \"crypto\",\n        \"cryptocurrency\",\n        \"forex\",\n        \"market-data\",\n        \"historical-data\",\n        \"live-data\",\n        \"ohlcv\",\n        \"pandas\",\n    ],\n)\n","path":null,"size_bytes":2419,"size_tokens":null},"README.md":{"content":"# XnoxsFetcher\n\n## Apa itu XnoxsFetcher?\n\n**XnoxsFetcher** adalah library Python yang memungkinkan Anda mengambil data pasar keuangan (saham, crypto, forex, komoditas) dari TradingView secara otomatis. Dengan library ini, Anda bisa mendapatkan data harga historis dan data real-time untuk analisis trading atau pembuatan bot trading.\n\n**Author:** developerxnoxs  \n**Versi:** 4.0.0  \n**Lisensi:** MIT\n\n---\n\n## Fitur Baru v4.0\n\n| Fitur | Deskripsi |\n|-------|-----------|\n| **Auth Manager** | Login dengan session persistence, auto token refresh |\n| **Data Cache** | Cache lokal SQLite, 4x lebih cepat untuk query berulang |\n| **Data Export** | Export ke CSV, Excel, JSON, Parquet |\n| **Parallel Fetch** | Ambil banyak simbol sekaligus, 5 simbol dalam ~1 detik |\n| **WebSocket Manager** | Auto-reconnect, heartbeat monitoring |\n\n---\n\n## Daftar Isi\n\n1. [Instalasi](#instalasi)\n2. [Penggunaan Dasar](#penggunaan-dasar)\n3. [Fitur Baru v4.0](#fitur-baru-v40-detail)\n   - [Auth Manager](#1-auth-manager)\n   - [Data Cache](#2-data-cache)\n   - [Data Export](#3-data-export)\n   - [Parallel Fetch](#4-parallel-fetch)\n4. [Mengambil Data Saham](#mengambil-data-saham)\n5. [Mengambil Data Cryptocurrency](#mengambil-data-cryptocurrency)\n6. [Memilih Timeframe](#memilih-timeframe)\n7. [Data Futures (Kontrak Berjangka)](#data-futures-kontrak-berjangka)\n8. [Extended Trading Session](#extended-trading-session)\n9. [Live Data Streaming](#live-data-streaming)\n10. [Autentikasi TradingView](#autentikasi-tradingview)\n11. [Daftar Exchange yang Didukung](#daftar-exchange-yang-didukung)\n12. [Troubleshooting](#troubleshooting)\n13. [Contoh Lengkap](#contoh-lengkap)\n\n---\n\n## Instalasi\n\n### Langkah 1: Pastikan Python Terinstall\n\nBuka terminal/command prompt dan ketik:\n\n```bash\npython --version\n```\n\nJika muncul versi Python (minimal 3.9), lanjut ke langkah 2. Jika tidak, install Python terlebih dahulu dari [python.org](https://python.org).\n\n### Langkah 2: Install Dependencies\n\n```bash\npip install pandas websocket-client requests python-dateutil openpyxl pyarrow\n```\n\n### Langkah 3: Download Library\n\n**Cara 1: Clone dari GitHub**\n```bash\ngit clone https://github.com/developerxnoxs/xnoxs_fetcher.git\ncd xnoxs_fetcher\npip install -e .\n```\n\n**Cara 2: Install langsung dari GitHub**\n```bash\npip install git+https://github.com/developerxnoxs/xnoxs_fetcher.git\n```\n\n### Langkah 4: Verifikasi Instalasi\n\nBuat file Python baru dan jalankan:\n\n```python\nfrom xnoxs_fetcher import XnoxsFetcher, TimeFrame\nprint(\"Instalasi berhasil!\")\n```\n\nJika tidak ada error, library siap digunakan.\n\n---\n\n## Penggunaan Dasar\n\n### Membuat Fetcher\n\n```python\nfrom xnoxs_fetcher import XnoxsFetcher, TimeFrame\n\n# Buat instance fetcher (tanpa login - mode anonymous)\nfetcher = XnoxsFetcher()\n```\n\n### Mengambil Data\n\n```python\n# Ambil 10 bar data harian Apple\ndata = fetcher.get_historical_data(\n    symbol=\"AAPL\",           # Kode saham\n    exchange=\"NASDAQ\",       # Nama bursa\n    timeframe=TimeFrame.DAILY,  # Timeframe harian\n    bars=10                  # Jumlah bar yang diambil\n)\n\n# Tampilkan data\nprint(data)\n```\n\n### Hasil Output\n\n```\n                          symbol     open    high       low   close      volume\ndatetime                                                                       \n2025-11-17 14:30:00  NASDAQ:AAPL  268.815  270.49  265.7300  267.46  45018260.0\n2025-11-18 14:30:00  NASDAQ:AAPL  269.990  270.71  265.3200  267.44  45677278.0\n...\n```\n\n**Penjelasan kolom:**\n- `datetime`: Waktu bar\n- `symbol`: Kode simbol dengan exchange\n- `open`: Harga pembukaan\n- `high`: Harga tertinggi\n- `low`: Harga terendah\n- `close`: Harga penutupan\n- `volume`: Volume perdagangan\n\n---\n\n## Fitur Baru v4.0 (Detail)\n\n### 1. Auth Manager\n\nAuth Manager menyediakan manajemen session yang lebih baik dengan fitur:\n- **Session Persistence**: Simpan session ke file, tidak perlu login ulang\n- **Auto Token Refresh**: Token diperbarui otomatis sebelum expired\n- **Rate Limit Protection**: Perlindungan dari rate limiting dengan retry otomatis\n\n```python\nfrom xnoxs_fetcher import AuthManager\n\n# Inisialisasi Auth Manager\nauth = AuthManager()\n\n# Login ke TradingView\ntoken = auth.authenticate(\"email@example.com\", \"password_anda\")\n\nif token:\n    print(\"Login berhasil!\")\n    \n    # Lihat info session\n    info = auth.get_session_info()\n    print(f\"Username: {info['username']}\")\n    print(f\"Expired: {info['is_expired']}\")\n    \n    # Session otomatis tersimpan ke file\n    # Login berikutnya akan menggunakan session yang tersimpan\n```\n\n**Menggunakan Session yang Tersimpan:**\n\n```python\nfrom xnoxs_fetcher import AuthManager\n\nauth = AuthManager()\n\n# Cek apakah ada session yang valid\ninfo = auth.get_session_info()\nif info['authenticated'] and not info['is_expired']:\n    print(\"Menggunakan session yang tersimpan\")\nelse:\n    # Login ulang jika session expired\n    auth.authenticate(\"email@example.com\", \"password_anda\")\n```\n\n---\n\n### 2. Data Cache\n\nData Cache menyimpan data yang sudah diambil ke database SQLite lokal, sehingga:\n- **4x Lebih Cepat**: Tidak perlu request ke server untuk data yang sama\n- **Hemat Bandwidth**: Mengurangi penggunaan internet\n- **Auto Expire**: Cache otomatis expired (default 24 jam, bisa diatur)\n- **Cek Kesegaran**: Bisa cek umur cache sebelum pakai\n\n```python\nfrom xnoxs_fetcher import XnoxsFetcher, DataCache, TimeFrame\n\n# Inisialisasi\nfetcher = XnoxsFetcher()\ncache = DataCache()  # Default folder: .tv_cache\n\n# Ambil data dari API\ndata = fetcher.get_historical_data(\n    symbol=\"AAPL\",\n    exchange=\"NASDAQ\",\n    timeframe=TimeFrame.DAILY,\n    bars=100\n)\n\n# Simpan ke cache (default TTL 24 jam)\ncache.set(\"AAPL\", \"NASDAQ\", \"1D\", 100, data)\n\n# Simpan dengan TTL custom (1 jam)\ncache.set(\"AAPL\", \"NASDAQ\", \"1D\", 100, data, ttl_hours=1)\n\n# Ambil dari cache (4x lebih cepat!)\ncached_data = cache.get(\"AAPL\", \"NASDAQ\", \"1D\", 100)\nif cached_data is not None:\n    print(\"Data dari cache:\", len(cached_data), \"rows\")\n```\n\n**Melihat Statistik Cache:**\n\n```python\nfrom xnoxs_fetcher import DataCache\n\ncache = DataCache()\n\n# Lihat statistik\nstats = cache.get_stats()\nprint(f\"Total entries: {stats['total_entries']}\")\nprint(f\"Valid entries: {stats['valid_entries']}\")\nprint(f\"Cache hits: {stats['total_cache_hits']}\")\nprint(f\"Size: {stats['database_size_mb']:.2f} MB\")\n\n# Lihat simbol yang ter-cache\nsymbols = cache.list_cached_symbols()\nfor s in symbols:\n    print(f\"  {s['symbol']}:{s['exchange']} - {s['timeframe']}\")\n```\n\n**Mengontrol Kesegaran Data (Penting!):**\n\n```python\nfrom xnoxs_fetcher import DataCache, CacheTTL\n\ncache = DataCache()\n\n# Cek umur cache sebelum pakai\nage = cache.get_cache_age(\"AAPL\", \"NASDAQ\", \"1D\", 100)\nif age:\n    print(f\"Cache berumur: {age['age_minutes']:.1f} menit\")\n    print(f\"Data masih fresh: {age['is_fresh']}\")  # < 1 jam = fresh\n\n# Cek apakah cache cukup segar (maksimal 30 menit)\nif cache.is_fresh(\"AAPL\", \"NASDAQ\", \"1D\", 100, max_age_minutes=30):\n    data = cache.get(\"AAPL\", \"NASDAQ\", \"1D\", 100)\nelse:\n    # Cache terlalu tua, ambil data baru dari API\n    data = fetcher.get_historical_data(\"AAPL\", \"NASDAQ\", TimeFrame.DAILY, 100)\n    cache.set(\"AAPL\", \"NASDAQ\", \"1D\", 100, data)\n```\n\n**Preset TTL untuk Berbagai Kebutuhan:**\n\n```python\nfrom xnoxs_fetcher import CacheTTL\n\n# CacheTTL menyediakan preset untuk berbagai use case:\n# CacheTTL.REALTIME = 0         # Tidak pakai cache\n# CacheTTL.MINUTES_5 = 5/60     # 5 menit - untuk scalping\n# CacheTTL.MINUTES_15 = 15/60   # 15 menit - untuk day trading\n# CacheTTL.HOUR_1 = 1           # 1 jam - untuk swing trading\n# CacheTTL.DAILY = 24           # 24 jam (default)\n# CacheTTL.WEEKLY = 168         # 1 minggu - untuk historical\n\n# Contoh: simpan dengan TTL 1 jam\ncache.set(\"AAPL\", \"NASDAQ\", \"1D\", 100, data, ttl_hours=CacheTTL.HOUR_1)\n```\n\n**Membersihkan Cache:**\n\n```python\n# Hapus cache untuk simbol tertentu\ncache.invalidate(symbol=\"AAPL\")\n\n# Hapus cache yang expired\ncache.cleanup_expired()\n\n# Hapus semua cache\ncache.clear_all()\n```\n\n---\n\n### 3. Data Export\n\nExport data ke berbagai format file:\n- **CSV**: Format universal, bisa dibuka di Excel\n- **Excel (.xlsx)**: Format native Excel dengan formatting\n- **JSON**: Untuk integrasi dengan aplikasi web\n- **Parquet**: Format optimal untuk data besar\n- **Text Report**: Laporan ringkasan dalam bentuk teks\n\n```python\nfrom xnoxs_fetcher import XnoxsFetcher, DataExporter, TimeFrame\n\n# Ambil data\nfetcher = XnoxsFetcher()\ndata = fetcher.get_historical_data(\n    symbol=\"AAPL\",\n    exchange=\"NASDAQ\",\n    timeframe=TimeFrame.DAILY,\n    bars=100\n)\n\n# Inisialisasi exporter\nexporter = DataExporter(output_dir=\"exports\")\n\n# Export ke CSV\ncsv_path = exporter.to_csv(data, \"AAPL_daily.csv\")\nprint(f\"Exported to: {csv_path}\")\n\n# Export ke Excel\nexcel_path = exporter.to_excel(data, \"AAPL_daily.xlsx\")\nprint(f\"Exported to: {excel_path}\")\n\n# Export ke JSON\njson_path = exporter.to_json(data, \"AAPL_daily.json\")\nprint(f\"Exported to: {json_path}\")\n\n# Export ke Parquet (untuk data besar)\nparquet_path = exporter.to_parquet(data, \"AAPL_daily.parquet\")\nprint(f\"Exported to: {parquet_path}\")\n\n# Buat laporan ringkasan\nreport_path = exporter.create_summary_report(data, \"AAPL_report.txt\")\nprint(f\"Report: {report_path}\")\n```\n\n**Contoh Output Laporan:**\n\n```\n============================================================\nMARKET DATA SUMMARY REPORT\nGenerated: 2025-12-02 16:15:26\n============================================================\n\nDATA OVERVIEW\n----------------------------------------\nTotal Records: 100\nDate Range: 2025-07-15 to 2025-12-02\nSymbols: NASDAQ:AAPL\n\nPRICE STATISTICS\n----------------------------------------\nOPEN: Min=165.50, Max=283.00, Mean=225.80\nHIGH: Min=168.25, Max=285.50, Mean=228.45\nLOW: Min=163.20, Max=280.10, Mean=223.15\nCLOSE: Min=165.80, Max=284.20, Mean=226.50\n```\n\n---\n\n### 4. Parallel Fetch\n\nAmbil data banyak simbol secara bersamaan dengan ThreadPoolExecutor:\n- **5x Lebih Cepat**: 5 simbol dalam ~1 detik vs ~5 detik secara sequential\n- **Progress Callback**: Monitor progress fetching\n- **Cache Integration**: Otomatis gunakan cache jika tersedia\n- **Error Handling**: Retry otomatis jika gagal\n\n```python\nfrom xnoxs_fetcher import XnoxsFetcher, fetch_parallel, TimeFrame\n\nfetcher = XnoxsFetcher()\n\n# Daftar simbol yang ingin diambil\nsymbols = [\n    (\"AAPL\", \"NASDAQ\"),\n    (\"GOOGL\", \"NASDAQ\"),\n    (\"MSFT\", \"NASDAQ\"),\n    (\"AMZN\", \"NASDAQ\"),\n    (\"META\", \"NASDAQ\"),\n]\n\n# Callback untuk progress (opsional)\ndef on_progress(symbol, exchange, success, current, total):\n    status = \"OK\" if success else \"FAILED\"\n    print(f\"[{current}/{total}] {symbol}:{exchange} - {status}\")\n\n# Fetch semua simbol secara parallel\nresults = fetch_parallel(\n    fetcher=fetcher,\n    symbols=symbols,\n    timeframe=TimeFrame.DAILY,\n    bars=100,\n    max_workers=5,  # Jumlah thread\n    progress_callback=on_progress\n)\n\n# Hasil berupa dictionary {(symbol, exchange): DataFrame}\nfor key, data in results.items():\n    symbol, exchange = key\n    if data is not None:\n        print(f\"{symbol}: {len(data)} rows, last price: ${data['close'].iloc[-1]:,.2f}\")\n```\n\n**Output:**\n```\n[1/5] GOOGL:NASDAQ - OK\n[2/5] AAPL:NASDAQ - OK\n[3/5] MSFT:NASDAQ - OK\n[4/5] AMZN:NASDAQ - OK\n[5/5] META:NASDAQ - OK\n\nGOOGL: 100 rows, last price: $178.35\nAAPL: 100 rows, last price: $234.50\nMSFT: 100 rows, last price: $425.80\nAMZN: 100 rows, last price: $205.25\nMETA: 100 rows, last price: $565.40\n```\n\n**Parallel Fetch dengan Cache:**\n\n```python\nfrom xnoxs_fetcher import XnoxsFetcher, DataCache, fetch_parallel, TimeFrame\n\nfetcher = XnoxsFetcher()\ncache = DataCache()\n\nsymbols = [\n    (\"AAPL\", \"NASDAQ\"),\n    (\"GOOGL\", \"NASDAQ\"),\n    (\"MSFT\", \"NASDAQ\"),\n]\n\n# Fetch dengan cache - data yang sudah ada di cache akan digunakan\nresults = fetch_parallel(\n    fetcher=fetcher,\n    symbols=symbols,\n    timeframe=TimeFrame.DAILY,\n    bars=100,\n    cache=cache,  # Gunakan cache\n    use_cache=True\n)\n\nprint(f\"Cache stats: {cache.get_stats()}\")\n```\n\n---\n\n## Mengambil Data Saham\n\n### Saham Amerika (NASDAQ, NYSE)\n\n```python\nfrom xnoxs_fetcher import XnoxsFetcher, TimeFrame\n\nfetcher = XnoxsFetcher()\n\n# Apple dari NASDAQ\napple = fetcher.get_historical_data(\n    symbol=\"AAPL\",\n    exchange=\"NASDAQ\",\n    timeframe=TimeFrame.DAILY,\n    bars=100\n)\n\n# Tesla dari NASDAQ\ntesla = fetcher.get_historical_data(\n    symbol=\"TSLA\",\n    exchange=\"NASDAQ\",\n    timeframe=TimeFrame.HOUR_1,\n    bars=50\n)\n\n# Microsoft dari NASDAQ\nmsft = fetcher.get_historical_data(\n    symbol=\"MSFT\",\n    exchange=\"NASDAQ\",\n    timeframe=TimeFrame.MINUTE_15,\n    bars=200\n)\n```\n\n### Saham Indonesia (IDX)\n\n```python\n# Bank BCA\nbbca = fetcher.get_historical_data(\n    symbol=\"BBCA\",\n    exchange=\"IDX\",\n    timeframe=TimeFrame.DAILY,\n    bars=100\n)\n\n# Telkom\ntlkm = fetcher.get_historical_data(\n    symbol=\"TLKM\",\n    exchange=\"IDX\",\n    timeframe=TimeFrame.DAILY,\n    bars=100\n)\n```\n\n### Saham India (NSE, BSE)\n\n```python\n# Reliance Industries\nreliance = fetcher.get_historical_data(\n    symbol=\"RELIANCE\",\n    exchange=\"NSE\",\n    timeframe=TimeFrame.DAILY,\n    bars=100\n)\n\n# Tata Consultancy Services\ntcs = fetcher.get_historical_data(\n    symbol=\"TCS\",\n    exchange=\"NSE\",\n    timeframe=TimeFrame.HOUR_4,\n    bars=200\n)\n```\n\n---\n\n## Mengambil Data Cryptocurrency\n\n### Bitcoin\n\n```python\nfrom xnoxs_fetcher import XnoxsFetcher, TimeFrame\n\nfetcher = XnoxsFetcher()\n\n# Bitcoin vs USD dari Binance\nbtc = fetcher.get_historical_data(\n    symbol=\"BTCUSD\",\n    exchange=\"BINANCE\",\n    timeframe=TimeFrame.HOUR_1,\n    bars=500\n)\n\nprint(f\"Harga BTC terakhir: ${btc['close'].iloc[-1]:,.2f}\")\n```\n\n### Ethereum\n\n```python\n# Ethereum vs USDT dari Binance\neth = fetcher.get_historical_data(\n    symbol=\"ETHUSDT\",\n    exchange=\"BINANCE\",\n    timeframe=TimeFrame.MINUTE_15,\n    bars=1000\n)\n\nprint(f\"Harga ETH terakhir: ${eth['close'].iloc[-1]:,.2f}\")\n```\n\n### Altcoins Lainnya\n\n```python\n# Solana\nsol = fetcher.get_historical_data(\n    symbol=\"SOLUSDT\",\n    exchange=\"BINANCE\",\n    timeframe=TimeFrame.HOUR_4,\n    bars=200\n)\n\n# Cardano\nada = fetcher.get_historical_data(\n    symbol=\"ADAUSDT\",\n    exchange=\"BINANCE\",\n    timeframe=TimeFrame.DAILY,\n    bars=100\n)\n\n# XRP\nxrp = fetcher.get_historical_data(\n    symbol=\"XRPUSDT\",\n    exchange=\"BINANCE\",\n    timeframe=TimeFrame.HOUR_1,\n    bars=300\n)\n```\n\n---\n\n## Memilih Timeframe\n\nTimeframe menentukan interval waktu setiap candlestick/bar. Berikut semua timeframe yang tersedia:\n\n### Tabel Timeframe\n\n| Kategori | TimeFrame | Penjelasan |\n|----------|-----------|------------|\n| **Menit** | `TimeFrame.MINUTE_1` | 1 menit per bar |\n| | `TimeFrame.MINUTE_3` | 3 menit per bar |\n| | `TimeFrame.MINUTE_5` | 5 menit per bar |\n| | `TimeFrame.MINUTE_15` | 15 menit per bar |\n| | `TimeFrame.MINUTE_30` | 30 menit per bar |\n| | `TimeFrame.MINUTE_45` | 45 menit per bar |\n| **Jam** | `TimeFrame.HOUR_1` | 1 jam per bar |\n| | `TimeFrame.HOUR_2` | 2 jam per bar |\n| | `TimeFrame.HOUR_3` | 3 jam per bar |\n| | `TimeFrame.HOUR_4` | 4 jam per bar |\n| **Harian+** | `TimeFrame.DAILY` | 1 hari per bar |\n| | `TimeFrame.WEEKLY` | 1 minggu per bar |\n| | `TimeFrame.MONTHLY` | 1 bulan per bar |\n\n### Contoh Penggunaan Berbagai Timeframe\n\n```python\nfrom xnoxs_fetcher import XnoxsFetcher, TimeFrame\n\nfetcher = XnoxsFetcher()\n\n# Data 1 menit - untuk scalping\ndata_1m = fetcher.get_historical_data(\n    symbol=\"BTCUSD\",\n    exchange=\"BINANCE\",\n    timeframe=TimeFrame.MINUTE_1,\n    bars=500\n)\n\n# Data 4 jam - untuk swing trading\ndata_4h = fetcher.get_historical_data(\n    symbol=\"BTCUSD\",\n    exchange=\"BINANCE\",\n    timeframe=TimeFrame.HOUR_4,\n    bars=200\n)\n\n# Data harian - untuk investasi jangka panjang\ndata_daily = fetcher.get_historical_data(\n    symbol=\"BTCUSD\",\n    exchange=\"BINANCE\",\n    timeframe=TimeFrame.DAILY,\n    bars=365  # 1 tahun terakhir\n)\n\n# Data mingguan - untuk analisis makro\ndata_weekly = fetcher.get_historical_data(\n    symbol=\"BTCUSD\",\n    exchange=\"BINANCE\",\n    timeframe=TimeFrame.WEEKLY,\n    bars=52  # 1 tahun terakhir\n)\n```\n\n---\n\n## Data Futures (Kontrak Berjangka)\n\nUntuk mengambil data futures/kontrak berjangka, gunakan parameter `futures_contract`:\n\n- `futures_contract=1` : Kontrak bulan depan (front month)\n- `futures_contract=2` : Kontrak bulan berikutnya (next month)\n\n### Contoh Futures\n\n```python\nfrom xnoxs_fetcher import XnoxsFetcher, TimeFrame\n\nfetcher = XnoxsFetcher()\n\n# Crude Oil Futures - front month\ncrude_oil = fetcher.get_historical_data(\n    symbol=\"CRUDEOIL\",\n    exchange=\"MCX\",\n    timeframe=TimeFrame.HOUR_4,\n    bars=100,\n    futures_contract=1  # Kontrak bulan depan\n)\n\n# Nifty Futures - front month\nnifty_fut = fetcher.get_historical_data(\n    symbol=\"NIFTY\",\n    exchange=\"NSE\",\n    timeframe=TimeFrame.MINUTE_15,\n    bars=200,\n    futures_contract=1\n)\n\n# Gold Futures - next month\ngold = fetcher.get_historical_data(\n    symbol=\"GOLD\",\n    exchange=\"MCX\",\n    timeframe=TimeFrame.HOUR_1,\n    bars=150,\n    futures_contract=2  # Kontrak bulan berikutnya\n)\n```\n\n---\n\n## Extended Trading Session\n\nUntuk mendapatkan data pre-market dan after-hours (sesi perdagangan diperpanjang), gunakan parameter `extended_session=True`:\n\n```python\nfrom xnoxs_fetcher import XnoxsFetcher, TimeFrame\n\nfetcher = XnoxsFetcher()\n\n# Data Apple termasuk pre-market dan after-hours\napple_extended = fetcher.get_historical_data(\n    symbol=\"AAPL\",\n    exchange=\"NASDAQ\",\n    timeframe=TimeFrame.MINUTE_5,\n    bars=500,\n    extended_session=True  # Termasuk sesi diperpanjang\n)\n\n# Data Tesla - hanya jam perdagangan reguler\ntesla_regular = fetcher.get_historical_data(\n    symbol=\"TSLA\",\n    exchange=\"NASDAQ\",\n    timeframe=TimeFrame.MINUTE_5,\n    bars=500,\n    extended_session=False  # Default: hanya jam reguler\n)\n```\n\n---\n\n## Live Data Streaming\n\nUntuk mendapatkan data real-time yang terus diupdate, gunakan `XnoxsLiveFeed`:\n\n### Contoh Live Streaming\n\n```python\nfrom xnoxs_fetcher import XnoxsLiveFeed, TimeFrame\nimport time\n\n# Buat live feed\nlive = XnoxsLiveFeed()\n\n# Buat symbol set untuk Bitcoin 1 menit\nseis = live.create_symbol_set(\n    symbol=\"BTCUSD\",\n    exchange=\"BINANCE\",\n    interval=TimeFrame.MINUTE_1\n)\n\n# Definisikan callback - fungsi yang dipanggil setiap ada data baru\ndef on_new_price(symbol_set, data):\n    close_price = data['close'].iloc[0]\n    timestamp = data.index[0]\n    print(f\"[{timestamp}] BTC: ${close_price:,.2f}\")\n\n# Daftarkan consumer\nconsumer = seis.create_consumer(on_new_price)\n\n# Biarkan streaming berjalan selama 5 menit\nprint(\"Streaming data BTC... (tekan Ctrl+C untuk berhenti)\")\ntry:\n    time.sleep(300)  # 5 menit\nexcept KeyboardInterrupt:\n    pass\n\n# Berhenti streaming\nlive.shutdown()\nprint(\"Streaming dihentikan.\")\n```\n\n### Streaming Multiple Symbols\n\n```python\nfrom xnoxs_fetcher import XnoxsLiveFeed, TimeFrame\n\nlive = XnoxsLiveFeed()\n\n# Callback untuk BTC\ndef on_btc_update(seis, data):\n    print(f\"BTC: ${data['close'].iloc[0]:,.2f}\")\n\n# Callback untuk ETH\ndef on_eth_update(seis, data):\n    print(f\"ETH: ${data['close'].iloc[0]:,.2f}\")\n\n# Buat symbol sets\nbtc_seis = live.create_symbol_set(\"BTCUSD\", \"BINANCE\", TimeFrame.MINUTE_1)\neth_seis = live.create_symbol_set(\"ETHUSDT\", \"BINANCE\", TimeFrame.MINUTE_1)\n\n# Daftarkan consumers\nbtc_consumer = btc_seis.create_consumer(on_btc_update)\neth_consumer = eth_seis.create_consumer(on_eth_update)\n\n# Streaming akan berjalan di background\n# Panggil live.shutdown() untuk menghentikan\n```\n\n---\n\n## Autentikasi TradingView\n\nUntuk akses data yang lebih lengkap, Anda bisa login dengan akun TradingView:\n\n### Cara 1: Langsung di XnoxsFetcher\n\n```python\nfrom xnoxs_fetcher import XnoxsFetcher, TimeFrame\n\n# Dengan login TradingView\nfetcher = XnoxsFetcher(\n    username=\"email_tradingview_anda@gmail.com\",\n    password=\"password_tradingview_anda\"\n)\n\n# Sekarang bisa mengakses data premium\ndata = fetcher.get_historical_data(\n    symbol=\"AAPL\",\n    exchange=\"NASDAQ\",\n    timeframe=TimeFrame.MINUTE_1,\n    bars=5000  # Bisa ambil lebih banyak data\n)\n```\n\n### Cara 2: Menggunakan Auth Manager (Recommended)\n\n```python\nfrom xnoxs_fetcher import XnoxsFetcher, AuthManager, TimeFrame\n\n# Login dengan Auth Manager\nauth = AuthManager()\ntoken = auth.authenticate(\"email@example.com\", \"password\")\n\nif token:\n    # Gunakan token di fetcher\n    fetcher = XnoxsFetcher(auth_token=token)\n    \n    data = fetcher.get_historical_data(\n        symbol=\"AAPL\",\n        exchange=\"NASDAQ\",\n        timeframe=TimeFrame.DAILY,\n        bars=100\n    )\n```\n\n### Cara 3: Menggunakan Environment Variables\n\n```bash\n# Set environment variables\nexport TRADINGVIEW_USERNAME=\"email@example.com\"\nexport TRADINGVIEW_PASSWORD=\"password_anda\"\n```\n\n```python\nimport os\nfrom xnoxs_fetcher import XnoxsFetcher, TimeFrame\n\n# Fetcher akan otomatis membaca dari environment\nfetcher = XnoxsFetcher(\n    username=os.environ.get('TRADINGVIEW_USERNAME'),\n    password=os.environ.get('TRADINGVIEW_PASSWORD')\n)\n```\n\n**Catatan:** Mode anonymous (tanpa login) juga berfungsi, tapi beberapa data mungkin terbatas.\n\n---\n\n## Daftar Exchange yang Didukung\n\nBerikut beberapa exchange populer yang didukung:\n\n### Saham Amerika\n| Exchange | Kode | Contoh Simbol |\n|----------|------|---------------|\n| NASDAQ | `NASDAQ` | AAPL, TSLA, MSFT, GOOGL |\n| NYSE | `NYSE` | JPM, BAC, WMT, DIS |\n| AMEX | `AMEX` | SPY, GLD, SLV |\n\n### Cryptocurrency\n| Exchange | Kode | Contoh Simbol |\n|----------|------|---------------|\n| Binance | `BINANCE` | BTCUSD, ETHUSDT, BNBUSDT |\n| Coinbase | `COINBASE` | BTCUSD, ETHUSD |\n| Bybit | `BYBIT` | BTCUSDT, ETHUSDT |\n\n### Saham Asia\n| Exchange | Kode | Contoh Simbol |\n|----------|------|---------------|\n| Indonesia | `IDX` | BBCA, TLKM, BBRI |\n| India NSE | `NSE` | RELIANCE, TCS, INFY |\n| India BSE | `BSE` | RELIANCE, TCS |\n| Hong Kong | `HKEX` | 0700, 9988 |\n| Tokyo | `TSE` | 7203, 9984 |\n\n### Forex\n| Exchange | Kode | Contoh Simbol |\n|----------|------|---------------|\n| FX | `FX` | EURUSD, GBPUSD, USDJPY |\n| OANDA | `OANDA` | EURUSD, GBPUSD |\n\n### Komoditas\n| Exchange | Kode | Contoh Simbol |\n|----------|------|---------------|\n| MCX India | `MCX` | GOLD, SILVER, CRUDEOIL |\n| COMEX | `COMEX` | GC1!, SI1! |\n| NYMEX | `NYMEX` | CL1!, NG1! |\n\n---\n\n## Troubleshooting\n\n### Error: \"Symbol search blocked by TradingView\"\n\n**Penyebab:** TradingView membatasi pencarian simbol dari server.\n\n**Solusi:** Gunakan simbol yang sudah Anda ketahui langsung. Pencarian simbol bisa dilakukan di website TradingView.\n\n### Error: \"WebSocket error: Connection to remote host was lost\"\n\n**Penyebab:** Koneksi terputus saat mengambil data terlalu cepat.\n\n**Solusi:** Tambahkan jeda antar request atau gunakan Parallel Fetch:\n\n```python\n# Cara 1: Tambahkan jeda\nimport time\n\ndata1 = fetcher.get_historical_data(\"AAPL\", \"NASDAQ\", TimeFrame.DAILY, 10)\ntime.sleep(1)  # Tunggu 1 detik\ndata2 = fetcher.get_historical_data(\"TSLA\", \"NASDAQ\", TimeFrame.DAILY, 10)\n\n# Cara 2: Gunakan Parallel Fetch (lebih efisien)\nfrom xnoxs_fetcher import fetch_parallel\n\nsymbols = [(\"AAPL\", \"NASDAQ\"), (\"TSLA\", \"NASDAQ\")]\nresults = fetch_parallel(fetcher, symbols, TimeFrame.DAILY, bars=10)\n```\n\n### Error: Data return None\n\n**Penyebab:** Simbol atau exchange salah.\n\n**Solusi:** Pastikan kode simbol dan exchange benar. Cek di TradingView.com.\n\n### Tips Performance\n\n1. **Gunakan Cache** - Hindari request berulang untuk data yang sama\n2. **Gunakan Parallel Fetch** - Untuk mengambil banyak simbol sekaligus\n3. **Gunakan jumlah bars yang wajar** - Maksimal 5000 bars per request\n4. **Login untuk akses penuh** - Beberapa data terbatas di mode anonymous\n\n---\n\n## Contoh Lengkap\n\n### Contoh 1: Portfolio Tracker dengan Cache\n\n```python\nfrom xnoxs_fetcher import XnoxsFetcher, DataCache, DataExporter, fetch_parallel, TimeFrame\n\n# Setup\nfetcher = XnoxsFetcher()\ncache = DataCache()\nexporter = DataExporter(output_dir=\"portfolio_data\")\n\n# Portfolio\nportfolio = [\n    (\"AAPL\", \"NASDAQ\", 10),   # 10 shares Apple\n    (\"GOOGL\", \"NASDAQ\", 5),   # 5 shares Google\n    (\"MSFT\", \"NASDAQ\", 8),    # 8 shares Microsoft\n    (\"AMZN\", \"NASDAQ\", 3),    # 3 shares Amazon\n]\n\n# Fetch semua data dengan cache\nsymbols = [(s[0], s[1]) for s in portfolio]\nresults = fetch_parallel(\n    fetcher=fetcher,\n    symbols=symbols,\n    timeframe=TimeFrame.DAILY,\n    bars=30,\n    cache=cache,\n    use_cache=True\n)\n\n# Hitung nilai portfolio\nprint(\"=\" * 60)\nprint(\"PORTFOLIO TRACKER\")\nprint(\"=\" * 60)\n\ntotal_value = 0\nfor symbol, exchange, shares in portfolio:\n    data = results.get((symbol, exchange))\n    if data is not None:\n        price = data['close'].iloc[-1]\n        value = price * shares\n        total_value += value\n        print(f\"{symbol}: {shares} shares x ${price:,.2f} = ${value:,.2f}\")\n\nprint(\"-\" * 60)\nprint(f\"TOTAL PORTFOLIO VALUE: ${total_value:,.2f}\")\n\n# Export data\nfor key, data in results.items():\n    if data is not None:\n        symbol, exchange = key\n        exporter.to_csv(data, f\"{symbol}_data.csv\")\n\nprint(f\"\\nData exported to: portfolio_data/\")\nprint(f\"Cache stats: {cache.get_stats()}\")\n```\n\n### Contoh 2: Crypto Monitor dengan Export\n\n```python\nfrom xnoxs_fetcher import XnoxsFetcher, DataExporter, fetch_parallel, TimeFrame\n\nfetcher = XnoxsFetcher()\nexporter = DataExporter(output_dir=\"crypto_reports\")\n\n# Top cryptos\ncryptos = [\n    (\"BTCUSD\", \"BINANCE\"),\n    (\"ETHUSDT\", \"BINANCE\"),\n    (\"BNBUSDT\", \"BINANCE\"),\n    (\"SOLUSDT\", \"BINANCE\"),\n    (\"ADAUSDT\", \"BINANCE\"),\n]\n\n# Fetch semua\nresults = fetch_parallel(\n    fetcher=fetcher,\n    symbols=cryptos,\n    timeframe=TimeFrame.HOUR_4,\n    bars=100\n)\n\nprint(\"=\" * 70)\nprint(\"CRYPTO MARKET OVERVIEW\")\nprint(\"=\" * 70)\nprint(f\"{'Symbol':<12} {'Price':<15} {'24h High':<15} {'24h Low':<15}\")\nprint(\"-\" * 70)\n\nfor symbol, exchange in cryptos:\n    data = results.get((symbol, exchange))\n    if data is not None:\n        last_row = data.iloc[-1]\n        day_data = data.tail(6)  # Last 24 hours (4h * 6 = 24h)\n        \n        price = last_row['close']\n        high = day_data['high'].max()\n        low = day_data['low'].min()\n        \n        print(f\"{symbol:<12} ${price:<14,.2f} ${high:<14,.2f} ${low:<14,.2f}\")\n        \n        # Export each crypto\n        exporter.to_csv(data, f\"{symbol.replace('USDT', '')}_4h.csv\")\n        exporter.create_summary_report(data, f\"{symbol.replace('USDT', '')}_report.txt\")\n\nprint(\"-\" * 70)\nprint(f\"\\nReports saved to: crypto_reports/\")\n```\n\n### Contoh 3: Saham Indonesia dengan Analisis\n\n```python\nfrom xnoxs_fetcher import XnoxsFetcher, DataCache, DataExporter, fetch_parallel, TimeFrame\n\nfetcher = XnoxsFetcher()\ncache = DataCache()\nexporter = DataExporter(output_dir=\"idx_analysis\")\n\n# Saham blue chip Indonesia\nidx_stocks = [\n    (\"BBCA\", \"IDX\"),\n    (\"BBRI\", \"IDX\"),\n    (\"TLKM\", \"IDX\"),\n    (\"ASII\", \"IDX\"),\n    (\"UNVR\", \"IDX\"),\n]\n\n# Fetch dengan cache\nresults = fetch_parallel(\n    fetcher=fetcher,\n    symbols=idx_stocks,\n    timeframe=TimeFrame.DAILY,\n    bars=252,  # 1 tahun\n    cache=cache,\n    use_cache=True\n)\n\nprint(\"=\" * 80)\nprint(\"ANALISIS SAHAM INDONESIA - BLUE CHIP\")\nprint(\"=\" * 80)\nprint(f\"{'Saham':<10} {'Harga':<12} {'YTD Return':<15} {'Max':<12} {'Min':<12}\")\nprint(\"-\" * 80)\n\nfor symbol, exchange in idx_stocks:\n    data = results.get((symbol, exchange))\n    if data is not None:\n        first_price = data['close'].iloc[0]\n        last_price = data['close'].iloc[-1]\n        returns = ((last_price - first_price) / first_price) * 100\n        max_price = data['high'].max()\n        min_price = data['low'].min()\n        \n        print(f\"{symbol:<10} Rp{last_price:<11,.0f} {returns:>+.2f}%{'':<8} Rp{max_price:<11,.0f} Rp{min_price:<11,.0f}\")\n        \n        # Export ke Excel\n        exporter.to_excel(data, f\"{symbol}_analysis.xlsx\")\n\nprint(\"-\" * 80)\nprint(f\"\\nData exported to: idx_analysis/\")\n```\n\n---\n\n## Bantuan & Dukungan\n\nJika mengalami masalah:\n\n1. Pastikan semua dependencies terinstall dengan benar\n2. Cek koneksi internet\n3. Verifikasi simbol dan exchange di TradingView.com\n4. Gunakan cache untuk mengurangi request berulang\n5. Gunakan parallel fetch untuk efisiensi\n\n**Repository:** https://github.com/developerxnoxs/xnoxs_fetcher\n\n---\n\n## Lisensi\n\nMIT License - Bebas digunakan untuk keperluan pribadi maupun komersial.\n\n---\n\n**Selamat trading! - developerxnoxs**\n","path":null,"size_bytes":27955,"size_tokens":null},"xnoxs_fetcher/core.py":{"content":"\"\"\"\nXnoxsFetcher Core Module\n\nThis module contains the main XnoxsFetcher class for retrieving\nhistorical market data from TradingView.\n\nAuthor: developerxnoxs\n\"\"\"\n\nfrom __future__ import annotations\n\nimport datetime\nimport enum\nimport json\nimport logging\nimport random\nimport re\nimport string\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Any, Dict, List, Union\n\nimport pandas as pd\nimport requests\nfrom websocket import create_connection, WebSocket\n\nlogger = logging.getLogger(__name__)\n\n\nclass TimeFrame(enum.Enum):\n    \"\"\"\n    Enumeration of supported chart timeframes.\n    \n    Each value represents the interval string used by TradingView's API.\n    \"\"\"\n    MINUTE_1 = \"1\"\n    MINUTE_3 = \"3\"\n    MINUTE_5 = \"5\"\n    MINUTE_15 = \"15\"\n    MINUTE_30 = \"30\"\n    MINUTE_45 = \"45\"\n    HOUR_1 = \"1H\"\n    HOUR_2 = \"2H\"\n    HOUR_3 = \"3H\"\n    HOUR_4 = \"4H\"\n    DAILY = \"1D\"\n    WEEKLY = \"1W\"\n    MONTHLY = \"1M\"\n    \n    @classmethod\n    def from_string(cls, value: str) -> \"TimeFrame\":\n        \"\"\"\n        Convert a string representation to TimeFrame enum.\n        \n        Args:\n            value: String representation of timeframe\n            \n        Returns:\n            Corresponding TimeFrame enum value\n            \n        Raises:\n            ValueError: If no matching timeframe found\n        \"\"\"\n        mapping = {\n            \"1\": cls.MINUTE_1, \"1m\": cls.MINUTE_1,\n            \"3\": cls.MINUTE_3, \"3m\": cls.MINUTE_3,\n            \"5\": cls.MINUTE_5, \"5m\": cls.MINUTE_5,\n            \"15\": cls.MINUTE_15, \"15m\": cls.MINUTE_15,\n            \"30\": cls.MINUTE_30, \"30m\": cls.MINUTE_30,\n            \"45\": cls.MINUTE_45, \"45m\": cls.MINUTE_45,\n            \"1h\": cls.HOUR_1, \"1H\": cls.HOUR_1,\n            \"2h\": cls.HOUR_2, \"2H\": cls.HOUR_2,\n            \"3h\": cls.HOUR_3, \"3H\": cls.HOUR_3,\n            \"4h\": cls.HOUR_4, \"4H\": cls.HOUR_4,\n            \"1d\": cls.DAILY, \"1D\": cls.DAILY, \"d\": cls.DAILY,\n            \"1w\": cls.WEEKLY, \"1W\": cls.WEEKLY, \"w\": cls.WEEKLY,\n            \"1M\": cls.MONTHLY, \"M\": cls.MONTHLY,\n        }\n        if value in mapping:\n            return mapping[value]\n        raise ValueError(f\"Unknown timeframe: {value}\")\n\n\n@dataclass\nclass FetcherConfig:\n    \"\"\"Configuration for XnoxsFetcher.\"\"\"\n    ws_timeout: int = 5\n    ws_debug: bool = False\n    sign_in_url: str = \"https://www.tradingview.com/accounts/signin/\"\n    search_url: str = \"https://symbol-search.tradingview.com/symbol_search/?text={}&hl=1&exchange={}&lang=en&type=&domain=production\"\n    ws_origin: str = \"https://data.tradingview.com\"\n    ws_endpoint: str = \"wss://data.tradingview.com/socket.io/websocket\"\n\n\nclass XnoxsFetcher:\n    \"\"\"\n    Advanced TradingView Historical Data Fetcher.\n    \n    This class provides methods to authenticate with TradingView,\n    search for symbols, and retrieve historical OHLCV data.\n    \n    Attributes:\n        token: Authentication token for TradingView API\n        session: WebSocket session identifier\n        chart_session: Chart session identifier\n        \n    Example:\n        >>> fetcher = XnoxsFetcher()\n        >>> data = fetcher.get_historical_data(\"AAPL\", \"NASDAQ\", TimeFrame.DAILY, 100)\n        >>> print(data.head())\n    \n    Author: developerxnoxs\n    \"\"\"\n    \n    __slots__ = (\n        \"_config\", \"_token\", \"_ws\", \"_session\", \n        \"_chart_session\", \"_ws_debug\"\n    )\n    \n    def __init__(\n        self,\n        username: Optional[str] = None,\n        password: Optional[str] = None,\n        config: Optional[FetcherConfig] = None\n    ) -> None:\n        \"\"\"\n        Initialize XnoxsFetcher instance.\n        \n        Args:\n            username: TradingView account username (optional)\n            password: TradingView account password (optional)  \n            config: Custom configuration (optional)\n        \"\"\"\n        self._config = config or FetcherConfig()\n        self._ws_debug = self._config.ws_debug\n        self._ws: Optional[WebSocket] = None\n        \n        self._token = self._authenticate(username, password)\n        \n        if self._token is None:\n            self._token = \"unauthorized_user_token\"\n            logger.warning(\n                \"Using anonymous mode - data access may be limited\"\n            )\n        \n        self._session = self._create_session_id()\n        self._chart_session = self._create_chart_session_id()\n    \n    @property\n    def token(self) -> str:\n        \"\"\"Get authentication token.\"\"\"\n        return self._token\n    \n    @property\n    def session(self) -> str:\n        \"\"\"Get session identifier.\"\"\"\n        return self._session\n    \n    @property\n    def chart_session(self) -> str:\n        \"\"\"Get chart session identifier.\"\"\"\n        return self._chart_session\n    \n    @property\n    def ws_debug(self) -> bool:\n        \"\"\"Get WebSocket debug mode status.\"\"\"\n        return self._ws_debug\n    \n    @ws_debug.setter \n    def ws_debug(self, value: bool) -> None:\n        \"\"\"Set WebSocket debug mode.\"\"\"\n        self._ws_debug = value\n    \n    def _authenticate(\n        self, \n        username: Optional[str], \n        password: Optional[str]\n    ) -> Optional[str]:\n        \"\"\"\n        Authenticate with TradingView.\n        \n        Args:\n            username: TradingView username\n            password: TradingView password\n            \n        Returns:\n            Authentication token or None if failed\n        \"\"\"\n        if username is None or password is None:\n            return None\n        \n        headers = {\n            \"Host\": \"www.tradingview.com\",\n            \"Origin\": \"https://www.tradingview.com\",\n            \"Referer\": \"https://www.tradingview.com/\",\n            \"User-Agent\": \"Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.7444.102 Mobile Safari/537.36\",\n            \"Accept\": \"*/*\",\n            \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n            \"Accept-Language\": \"id,id-ID;q=0.9,en-US;q=0.8,en;q=0.7\",\n            \"x-language\": \"en\",\n            \"x-requested-with\": \"XMLHttpRequest\",\n            \"sec-ch-ua\": '\"Chromium\";v=\"142\", \"Android WebView\";v=\"142\", \"Not_A Brand\";v=\"99\"',\n            \"sec-ch-ua-mobile\": \"?1\",\n            \"sec-ch-ua-platform\": '\"Android\"',\n            \"sec-fetch-dest\": \"empty\",\n            \"sec-fetch-mode\": \"same-origin\",\n            \"sec-fetch-site\": \"same-origin\",\n        }\n        \n        files = {\n            \"username\": (None, username),\n            \"password\": (None, password),\n            \"remember\": (None, \"true\"),\n        }\n        \n        try:\n            session = requests.Session()\n            \n            session.cookies.set(\"cookiePrivacyPreferenceBannerProduction\", \"notApplicable\", domain=\".tradingview.com\")\n            session.cookies.set(\"cookiesSettings\", '{\"analytics\":true,\"advertising\":true}', domain=\".tradingview.com\")\n            \n            response = session.post(\n                self._config.sign_in_url,\n                files=files,\n                headers=headers,\n                timeout=15\n            )\n            response.raise_for_status()\n            \n            result = response.json()\n            \n            if result.get(\"error\"):\n                logger.error(f\"Login error: {result['error']}\")\n                return None\n            \n            user_data = result.get(\"user\", {})\n            \n            if \"auth_token\" in user_data:\n                logger.info(f\"Login successful for user: {user_data.get('username')}\")\n                return user_data[\"auth_token\"]\n            \n            if \"sessionid\" in session.cookies:\n                logger.info(f\"Login successful (session-based) for user: {user_data.get('username')}\")\n                return session.cookies.get(\"sessionid\")\n            \n            logger.warning(\"Login succeeded but no auth token found, using session cookies\")\n            return f\"session:{session.cookies.get('sessionid', 'unknown')}\"\n            \n        except requests.exceptions.RequestException as exc:\n            logger.error(f\"Authentication request failed: {exc}\")\n            return None\n        except (KeyError, ValueError) as exc:\n            logger.error(f\"Authentication failed to parse response: {exc}\")\n            return None\n    \n    def _establish_websocket(self) -> None:\n        \"\"\"Establish WebSocket connection to TradingView.\"\"\"\n        logger.debug(\"Establishing WebSocket connection...\")\n        ws_headers = json.dumps({\"Origin\": self._config.ws_origin})\n        self._ws = create_connection(\n            self._config.ws_endpoint,\n            headers=ws_headers,\n            timeout=self._config.ws_timeout\n        )\n    \n    @staticmethod\n    def _create_session_id(length: int = 12) -> str:\n        \"\"\"Generate a random session identifier.\"\"\"\n        chars = string.ascii_lowercase\n        random_part = \"\".join(random.choice(chars) for _ in range(length))\n        return f\"qs_{random_part}\"\n    \n    @staticmethod\n    def _create_chart_session_id(length: int = 12) -> str:\n        \"\"\"Generate a random chart session identifier.\"\"\"\n        chars = string.ascii_lowercase\n        random_part = \"\".join(random.choice(chars) for _ in range(length))\n        return f\"cs_{random_part}\"\n    \n    @staticmethod\n    def _add_header(message: str) -> str:\n        \"\"\"Add TradingView message header.\"\"\"\n        return f\"~m~{len(message)}~m~{message}\"\n    \n    @staticmethod\n    def _build_message(func_name: str, params: List[Any]) -> str:\n        \"\"\"Build JSON message for WebSocket.\"\"\"\n        return json.dumps({\"m\": func_name, \"p\": params}, separators=(\",\", \":\"))\n    \n    def _create_ws_message(self, func_name: str, params: List[Any]) -> str:\n        \"\"\"Create complete WebSocket message with header.\"\"\"\n        return self._add_header(self._build_message(func_name, params))\n    \n    def _send_ws_message(self, func_name: str, params: List[Any]) -> None:\n        \"\"\"Send message through WebSocket.\"\"\"\n        message = self._create_ws_message(func_name, params)\n        if self._ws_debug:\n            print(f\"[DEBUG] Sending: {message}\")\n        self._ws.send(message)\n    \n    @staticmethod\n    def _parse_raw_data(raw_data: str, symbol: str) -> Optional[pd.DataFrame]:\n        \"\"\"\n        Parse raw WebSocket response into DataFrame.\n        \n        Args:\n            raw_data: Raw response string from WebSocket\n            symbol: Symbol name for labeling\n            \n        Returns:\n            DataFrame with OHLCV data or None if parsing failed\n        \"\"\"\n        try:\n            match = re.search(r'\"s\":\\[(.+?)\\}\\]', raw_data)\n            if not match:\n                return None\n                \n            data_str = match.group(1)\n            rows = data_str.split(',{\"')\n            parsed_data: List[List[Any]] = []\n            has_volume = True\n            \n            for row in rows:\n                parts = re.split(r\"\\[|:|,|\\]\", row)\n                timestamp = datetime.datetime.fromtimestamp(float(parts[4]))\n                \n                record = [timestamp]\n                for idx in range(5, 10):\n                    if not has_volume and idx == 9:\n                        record.append(0.0)\n                        continue\n                    try:\n                        record.append(float(parts[idx]))\n                    except (ValueError, IndexError):\n                        has_volume = False\n                        record.append(0.0)\n                        logger.debug(\"Volume data not available\")\n                \n                parsed_data.append(record)\n            \n            df = pd.DataFrame(\n                parsed_data,\n                columns=[\"datetime\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n            ).set_index(\"datetime\")\n            df.insert(0, \"symbol\", symbol)\n            \n            return df\n            \n        except AttributeError:\n            logger.error(\"Failed to parse data - check symbol and exchange\")\n            return None\n    \n    @staticmethod\n    def _format_symbol(\n        symbol: str, \n        exchange: str, \n        contract: Optional[int] = None\n    ) -> str:\n        \"\"\"\n        Format symbol string for TradingView API.\n        \n        Args:\n            symbol: Raw symbol name\n            exchange: Exchange name\n            contract: Futures contract number (optional)\n            \n        Returns:\n            Formatted symbol string\n        \"\"\"\n        if \":\" in symbol:\n            return symbol\n            \n        if contract is None:\n            return f\"{exchange}:{symbol}\"\n        \n        if isinstance(contract, int):\n            return f\"{exchange}:{symbol}{contract}!\"\n            \n        raise ValueError(f\"Invalid contract value: {contract}\")\n    \n    def get_historical_data(\n        self,\n        symbol: str,\n        exchange: str = \"NSE\",\n        timeframe: TimeFrame = TimeFrame.DAILY,\n        bars: int = 10,\n        futures_contract: Optional[int] = None,\n        extended_session: bool = False,\n    ) -> Optional[pd.DataFrame]:\n        \"\"\"\n        Retrieve historical OHLCV data from TradingView.\n        \n        Args:\n            symbol: Trading symbol (e.g., \"AAPL\", \"BTCUSD\")\n            exchange: Exchange name (e.g., \"NASDAQ\", \"BINANCE\")\n            timeframe: Chart timeframe (default: DAILY)\n            bars: Number of bars to retrieve (max: 5000)\n            futures_contract: Futures contract number (optional)\n                - None: Spot/cash\n                - 1: Current front month\n                - 2: Next front month\n            extended_session: Include extended trading hours\n            \n        Returns:\n            DataFrame with columns: symbol, open, high, low, close, volume\n            Returns None if data retrieval fails\n            \n        Example:\n            >>> fetcher = XnoxsFetcher()\n            >>> data = fetcher.get_historical_data(\n            ...     \"AAPL\", \"NASDAQ\", TimeFrame.HOUR_1, 100\n            ... )\n        \"\"\"\n        formatted_symbol = self._format_symbol(\n            symbol, exchange, futures_contract\n        )\n        interval_value = timeframe.value\n        \n        self._establish_websocket()\n        \n        self._send_ws_message(\"set_auth_token\", [self._token])\n        self._send_ws_message(\"chart_create_session\", [self._chart_session, \"\"])\n        self._send_ws_message(\"quote_create_session\", [self._session])\n        \n        quote_fields = [\n            self._session,\n            \"ch\", \"chp\", \"current_session\", \"description\", \"local_description\",\n            \"language\", \"exchange\", \"fractional\", \"is_tradable\", \"lp\", \"lp_time\",\n            \"minmov\", \"minmove2\", \"original_name\", \"pricescale\", \"pro_name\",\n            \"short_name\", \"type\", \"update_mode\", \"volume\", \"currency_code\",\n            \"rchp\", \"rtc\",\n        ]\n        self._send_ws_message(\"quote_set_fields\", quote_fields)\n        \n        self._send_ws_message(\n            \"quote_add_symbols\",\n            [self._session, formatted_symbol, {\"flags\": [\"force_permission\"]}]\n        )\n        self._send_ws_message(\"quote_fast_symbols\", [self._session, formatted_symbol])\n        \n        session_type = '\"extended\"' if extended_session else '\"regular\"'\n        resolve_payload = (\n            f'={{\"symbol\":\"{formatted_symbol}\",'\n            f'\"adjustment\":\"splits\",\"session\":{session_type}}}'\n        )\n        self._send_ws_message(\n            \"resolve_symbol\",\n            [self._chart_session, \"symbol_1\", resolve_payload]\n        )\n        \n        self._send_ws_message(\n            \"create_series\",\n            [self._chart_session, \"s1\", \"s1\", \"symbol_1\", interval_value, bars]\n        )\n        self._send_ws_message(\"switch_timezone\", [self._chart_session, \"exchange\"])\n        \n        raw_data = \"\"\n        logger.debug(f\"Fetching data for {formatted_symbol}...\")\n        \n        while True:\n            try:\n                result = self._ws.recv()\n                raw_data += result + \"\\n\"\n            except Exception as exc:\n                logger.error(f\"WebSocket error: {exc}\")\n                break\n            \n            if \"series_completed\" in result:\n                break\n        \n        return self._parse_raw_data(raw_data, formatted_symbol)\n    \n    def search_symbols(\n        self, \n        query: str, \n        exchange: str = \"\"\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Search for trading symbols on TradingView.\n        \n        Args:\n            query: Search query (e.g., \"AAPL\", \"Bitcoin\")\n            exchange: Filter by exchange (optional)\n            \n        Returns:\n            List of matching symbol dictionaries\n            \n        Example:\n            >>> fetcher = XnoxsFetcher()\n            >>> results = fetcher.search_symbols(\"AAPL\", \"NASDAQ\")\n            >>> for r in results[:3]:\n            ...     print(f\"{r['symbol']} - {r['description']}\")\n        \"\"\"\n        url = self._config.search_url.format(query, exchange)\n        \n        headers = {\n            \"Referer\": \"https://www.tradingview.com\",\n            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n            \"Accept\": \"application/json\",\n            \"Accept-Language\": \"en-US,en;q=0.9\",\n        }\n        \n        try:\n            response = requests.get(url, headers=headers, timeout=10)\n            \n            if response.status_code == 403:\n                logger.warning(\"Symbol search blocked by TradingView - try again later\")\n                return []\n                \n            response.raise_for_status()\n            \n            text = response.text.replace(\"</em>\", \"\").replace(\"<em>\", \"\")\n            return json.loads(text)\n            \n        except requests.exceptions.HTTPError as exc:\n            logger.warning(f\"Symbol search HTTP error: {exc}\")\n            return []\n        except Exception as exc:\n            logger.error(f\"Symbol search failed: {exc}\")\n            return []\n    \n    def search_symbol(\n        self,\n        text: str,\n        exchange: str = \"\"\n    ) -> List[Dict[str, Any]]:\n        \"\"\"\n        Search for trading symbols (backward compatibility alias).\n        \n        See search_symbols() for full documentation.\n        \"\"\"\n        return self.search_symbols(text, exchange)\n    \n    def get_hist(\n        self,\n        symbol: str,\n        exchange: str = \"NSE\",\n        interval: \"TimeFrame\" = None,\n        n_bars: int = 10,\n        fut_contract: Optional[int] = None,\n        extended_session: bool = False,\n    ) -> Optional[pd.DataFrame]:\n        \"\"\"\n        Get historical data (backward compatibility alias).\n        \n        See get_historical_data() for full documentation.\n        \"\"\"\n        if interval is None:\n            interval = TimeFrame.DAILY\n        return self.get_historical_data(\n            symbol=symbol,\n            exchange=exchange,\n            timeframe=interval,\n            bars=n_bars,\n            futures_contract=fut_contract,\n            extended_session=extended_session,\n        )\n\n\n# Backward compatibility aliases\nInterval = TimeFrame\nTvDatafeed = XnoxsFetcher\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.DEBUG)\n    \n    fetcher = XnoxsFetcher()\n    \n    print(\"\\n=== Crude Oil Futures ===\")\n    print(fetcher.get_historical_data(\"CRUDEOIL\", \"MCX\", futures_contract=1))\n    \n    print(\"\\n=== Nifty Futures ===\")\n    print(fetcher.get_historical_data(\"NIFTY\", \"NSE\", futures_contract=1))\n    \n    print(\"\\n=== Eicher Motors Hourly ===\")\n    print(fetcher.get_historical_data(\n        \"EICHERMOT\", \"NSE\",\n        timeframe=TimeFrame.HOUR_1,\n        bars=500,\n        extended_session=False\n    ))\n","path":null,"size_bytes":19474,"size_tokens":null},"demo.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nXnoxsFetcher Demo Script\n\nDemonstrates the capabilities of the xnoxs_fetcher library\nfor fetching market data from TradingView.\n\nAuthor: developerxnoxs\n\"\"\"\n\nfrom __future__ import annotations\n\nimport sys\nfrom typing import Optional\n\nimport pandas as pd\n\nfrom xnoxs_fetcher import XnoxsFetcher, TimeFrame\n\n\ndef print_header(title: str, char: str = \"=\") -> None:\n    \"\"\"Print formatted section header.\"\"\"\n    line = char * 60\n    print(f\"\\n{line}\")\n    print(f\"  {title}\")\n    print(f\"{line}\\n\")\n\n\ndef print_subheader(title: str) -> None:\n    \"\"\"Print formatted subsection header.\"\"\"\n    print(f\"\\n{'─' * 60}\")\n    print(f\"  {title}\")\n    print(f\"{'─' * 60}\\n\")\n\n\ndef display_dataframe(df: Optional[pd.DataFrame], name: str) -> bool:\n    \"\"\"Display dataframe with formatting.\"\"\"\n    if df is None or df.empty:\n        print(f\"  [ERROR] Failed to retrieve {name} data\")\n        return False\n    \n    print(df.to_string())\n    print(f\"\\n  [OK] Retrieved {len(df)} bars of {name} data\")\n    return True\n\n\ndef demo_basic_fetch(fetcher: XnoxsFetcher) -> None:\n    \"\"\"Demonstrate basic data fetching.\"\"\"\n    print_subheader(\"Basic Data Fetching - Apple (AAPL)\")\n    \n    data = fetcher.get_historical_data(\n        symbol=\"AAPL\",\n        exchange=\"NASDAQ\",\n        timeframe=TimeFrame.DAILY,\n        bars=10\n    )\n    display_dataframe(data, \"AAPL\")\n\n\ndef demo_crypto_fetch(fetcher: XnoxsFetcher) -> None:\n    \"\"\"Demonstrate cryptocurrency data fetching.\"\"\"\n    print_subheader(\"Cryptocurrency - Bitcoin (BTCUSD)\")\n    \n    data = fetcher.get_historical_data(\n        symbol=\"BTCUSD\",\n        exchange=\"BINANCE\",\n        timeframe=TimeFrame.HOUR_1,\n        bars=10\n    )\n    display_dataframe(data, \"BTCUSD\")\n\n\ndef demo_different_timeframes(fetcher: XnoxsFetcher) -> None:\n    \"\"\"Demonstrate different timeframe options.\"\"\"\n    print_subheader(\"Multiple Timeframes - Ethereum\")\n    \n    timeframes = [\n        (TimeFrame.MINUTE_15, \"15-minute\"),\n        (TimeFrame.HOUR_4, \"4-hour\"),\n        (TimeFrame.DAILY, \"Daily\"),\n    ]\n    \n    for tf, name in timeframes:\n        print(f\"\\n  {name} timeframe:\")\n        data = fetcher.get_historical_data(\n            symbol=\"ETHUSDT\",\n            exchange=\"BINANCE\",\n            timeframe=tf,\n            bars=5\n        )\n        if data is not None:\n            print(f\"    Latest close: ${data['close'].iloc[-1]:,.2f}\")\n            print(f\"    Volume: {data['volume'].iloc[-1]:,.0f}\")\n\n\ndef demo_symbol_search(fetcher: XnoxsFetcher) -> None:\n    \"\"\"Demonstrate symbol search functionality.\"\"\"\n    print_subheader(\"Symbol Search\")\n    \n    queries = [\n        (\"TSLA\", \"NASDAQ\"),\n        (\"BTC\", \"\"),\n    ]\n    \n    for query, exchange in queries:\n        filter_text = f\" on {exchange}\" if exchange else \"\"\n        print(f\"  Searching for '{query}'{filter_text}...\")\n        \n        results = fetcher.search_symbols(query, exchange)\n        \n        if results:\n            print(f\"  Found {len(results)} result(s):\")\n            for i, r in enumerate(results[:3], 1):\n                symbol = r.get(\"symbol\", \"N/A\")\n                desc = r.get(\"description\", \"N/A\")\n                exch = r.get(\"exchange\", \"N/A\")\n                print(f\"    {i}. {exch}:{symbol} - {desc}\")\n        else:\n            print(\"  No results found\")\n        print()\n\n\ndef demo_available_timeframes() -> None:\n    \"\"\"Display all available timeframes.\"\"\"\n    print_subheader(\"Available Timeframes\")\n    \n    print(\"  XnoxsFetcher supports the following timeframes:\\n\")\n    \n    categories = {\n        \"Minutes\": [TimeFrame.MINUTE_1, TimeFrame.MINUTE_3, TimeFrame.MINUTE_5,\n                   TimeFrame.MINUTE_15, TimeFrame.MINUTE_30, TimeFrame.MINUTE_45],\n        \"Hours\": [TimeFrame.HOUR_1, TimeFrame.HOUR_2, TimeFrame.HOUR_3, TimeFrame.HOUR_4],\n        \"Days+\": [TimeFrame.DAILY, TimeFrame.WEEKLY, TimeFrame.MONTHLY],\n    }\n    \n    for category, timeframes in categories.items():\n        tf_names = [tf.name for tf in timeframes]\n        print(f\"  {category}: {', '.join(tf_names)}\")\n\n\ndef main() -> int:\n    \"\"\"Run the demo.\"\"\"\n    print_header(\"XnoxsFetcher Demo\", \"═\")\n    print(\"  Advanced TradingView Data Fetcher\")\n    print(\"  Author: developerxnoxs\")\n    print(\"  Version: 3.0.0\")\n    \n    print(\"\\n  Initializing XnoxsFetcher...\")\n    print(\"  NOTE: Running in anonymous mode (limited data access)\")\n    \n    try:\n        fetcher = XnoxsFetcher()\n        print(\"  [OK] Fetcher initialized successfully\")\n    except Exception as exc:\n        print(f\"  [ERROR] Failed to initialize: {exc}\")\n        return 1\n    \n    demo_basic_fetch(fetcher)\n    demo_crypto_fetch(fetcher)\n    demo_different_timeframes(fetcher)\n    demo_symbol_search(fetcher)\n    demo_available_timeframes()\n    \n    print_header(\"Demo Complete\", \"═\")\n    print(\"  For authenticated access, initialize with credentials:\")\n    print(\"    fetcher = XnoxsFetcher(username='...', password='...')\")\n    print()\n    print(\"  For live data streaming, use XnoxsLiveFeed:\")\n    print(\"    from xnoxs_fetcher import XnoxsLiveFeed, TimeFrame\")\n    print()\n    \n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n","path":null,"size_bytes":5142,"size_tokens":null},"xnoxs_fetcher/auth.py":{"content":"\"\"\"\nXnoxsFetcher Authentication Module\n\nThis module provides robust authentication management for TradingView,\nincluding session persistence, token refresh, and rate limiting.\n\nAuthor: developerxnoxs\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport os\nimport time\nimport threading\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import Optional, Dict, Any\n\nimport requests\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass AuthConfig:\n    \"\"\"Configuration for authentication manager.\"\"\"\n    sign_in_url: str = \"https://www.tradingview.com/accounts/signin/\"\n    session_file: str = \".tv_session.json\"\n    token_refresh_interval: int = 3600\n    max_retries: int = 3\n    retry_delay: float = 2.0\n    rate_limit_requests: int = 10\n    rate_limit_window: int = 60\n\n\n@dataclass\nclass SessionData:\n    \"\"\"Stored session data.\"\"\"\n    token: str\n    session_id: str\n    username: str\n    created_at: datetime\n    expires_at: datetime\n    cookies: Dict[str, str] = field(default_factory=dict)\n    \n    def is_expired(self) -> bool:\n        \"\"\"Check if session has expired.\"\"\"\n        return datetime.now() >= self.expires_at\n    \n    def is_near_expiry(self, threshold_minutes: int = 30) -> bool:\n        \"\"\"Check if session is close to expiring.\"\"\"\n        threshold = timedelta(minutes=threshold_minutes)\n        return datetime.now() >= (self.expires_at - threshold)\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for serialization.\"\"\"\n        return {\n            \"token\": self.token,\n            \"session_id\": self.session_id,\n            \"username\": self.username,\n            \"created_at\": self.created_at.isoformat(),\n            \"expires_at\": self.expires_at.isoformat(),\n            \"cookies\": self.cookies\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> \"SessionData\":\n        \"\"\"Create from dictionary.\"\"\"\n        return cls(\n            token=data[\"token\"],\n            session_id=data[\"session_id\"],\n            username=data[\"username\"],\n            created_at=datetime.fromisoformat(data[\"created_at\"]),\n            expires_at=datetime.fromisoformat(data[\"expires_at\"]),\n            cookies=data.get(\"cookies\", {})\n        )\n\n\nclass RateLimiter:\n    \"\"\"Rate limiter to prevent API abuse.\"\"\"\n    \n    def __init__(self, max_requests: int = 10, window_seconds: int = 60):\n        self._max_requests = max_requests\n        self._window_seconds = window_seconds\n        self._requests: list = []\n        self._lock = threading.Lock()\n    \n    def acquire(self, timeout: float = 30.0) -> bool:\n        \"\"\"\n        Acquire permission to make a request.\n        \n        Args:\n            timeout: Maximum time to wait for permission\n            \n        Returns:\n            True if permission granted, False if timed out\n        \"\"\"\n        start_time = time.time()\n        \n        while True:\n            with self._lock:\n                now = time.time()\n                self._requests = [t for t in self._requests \n                                  if now - t < self._window_seconds]\n                \n                if len(self._requests) < self._max_requests:\n                    self._requests.append(now)\n                    return True\n            \n            if time.time() - start_time >= timeout:\n                return False\n            \n            time.sleep(0.1)\n    \n    def get_wait_time(self) -> float:\n        \"\"\"Get time until next request slot is available.\"\"\"\n        with self._lock:\n            if len(self._requests) < self._max_requests:\n                return 0.0\n            \n            oldest = min(self._requests)\n            return max(0.0, self._window_seconds - (time.time() - oldest))\n\n\nclass AuthManager:\n    \"\"\"\n    Robust TradingView Authentication Manager.\n    \n    Features:\n        - Session persistence to file\n        - Automatic token refresh\n        - Rate limiting\n        - Retry with exponential backoff\n        - Thread-safe operations\n    \n    Example:\n        >>> auth = AuthManager()\n        >>> token = auth.authenticate(\"user@email.com\", \"password\")\n        >>> if auth.is_authenticated:\n        ...     print(\"Login successful!\")\n    \n    Author: developerxnoxs\n    \"\"\"\n    \n    _HEADERS_TEMPLATE = {\n        \"Host\": \"www.tradingview.com\",\n        \"Origin\": \"https://www.tradingview.com\",\n        \"Referer\": \"https://www.tradingview.com/\",\n        \"User-Agent\": \"Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.7444.102 Mobile Safari/537.36\",\n        \"Accept\": \"*/*\",\n        \"Accept-Encoding\": \"gzip, deflate, br, zstd\",\n        \"Accept-Language\": \"id,id-ID;q=0.9,en-US;q=0.8,en;q=0.7\",\n        \"x-language\": \"en\",\n        \"x-requested-with\": \"XMLHttpRequest\",\n        \"sec-ch-ua\": '\"Chromium\";v=\"142\", \"Android WebView\";v=\"142\", \"Not_A Brand\";v=\"99\"',\n        \"sec-ch-ua-mobile\": \"?1\",\n        \"sec-ch-ua-platform\": '\"Android\"',\n        \"sec-fetch-dest\": \"empty\",\n        \"sec-fetch-mode\": \"same-origin\",\n        \"sec-fetch-site\": \"same-origin\",\n    }\n    \n    def __init__(self, config: Optional[AuthConfig] = None):\n        \"\"\"\n        Initialize AuthManager.\n        \n        Args:\n            config: Authentication configuration\n        \"\"\"\n        self._config = config or AuthConfig()\n        self._session_data: Optional[SessionData] = None\n        self._http_session: Optional[requests.Session] = None\n        self._rate_limiter = RateLimiter(\n            self._config.rate_limit_requests,\n            self._config.rate_limit_window\n        )\n        self._lock = threading.Lock()\n        self._refresh_thread: Optional[threading.Thread] = None\n        self._stop_refresh = threading.Event()\n        \n        self._load_session()\n    \n    @property\n    def is_authenticated(self) -> bool:\n        \"\"\"Check if currently authenticated.\"\"\"\n        return (\n            self._session_data is not None \n            and not self._session_data.is_expired()\n        )\n    \n    @property\n    def token(self) -> Optional[str]:\n        \"\"\"Get current authentication token.\"\"\"\n        if self._session_data:\n            return self._session_data.token\n        return None\n    \n    @property\n    def session_id(self) -> Optional[str]:\n        \"\"\"Get current session ID.\"\"\"\n        if self._session_data:\n            return self._session_data.session_id\n        return None\n    \n    @property\n    def username(self) -> Optional[str]:\n        \"\"\"Get authenticated username.\"\"\"\n        if self._session_data:\n            return self._session_data.username\n        return None\n    \n    def _get_session_path(self) -> Path:\n        \"\"\"Get path to session file.\"\"\"\n        return Path(self._config.session_file)\n    \n    def _load_session(self) -> bool:\n        \"\"\"\n        Load session from file if available.\n        \n        Returns:\n            True if valid session loaded\n        \"\"\"\n        session_path = self._get_session_path()\n        \n        if not session_path.exists():\n            return False\n        \n        try:\n            with open(session_path, \"r\") as f:\n                data = json.load(f)\n            \n            self._session_data = SessionData.from_dict(data)\n            \n            if self._session_data.is_expired():\n                logger.info(\"Stored session expired, need re-authentication\")\n                self._session_data = None\n                return False\n            \n            logger.info(f\"Loaded session for user: {self._session_data.username}\")\n            self._start_refresh_thread()\n            return True\n            \n        except (json.JSONDecodeError, KeyError, ValueError) as e:\n            logger.warning(f\"Failed to load session: {e}\")\n            return False\n    \n    def _save_session(self) -> None:\n        \"\"\"Save current session to file.\"\"\"\n        if self._session_data is None:\n            return\n        \n        session_path = self._get_session_path()\n        \n        try:\n            with open(session_path, \"w\") as f:\n                json.dump(self._session_data.to_dict(), f, indent=2)\n            logger.debug(\"Session saved to file\")\n        except IOError as e:\n            logger.warning(f\"Failed to save session: {e}\")\n    \n    def _clear_session(self) -> None:\n        \"\"\"Clear stored session.\"\"\"\n        self._session_data = None\n        \n        session_path = self._get_session_path()\n        if session_path.exists():\n            try:\n                session_path.unlink()\n            except IOError:\n                pass\n    \n    def authenticate(\n        self,\n        username: str,\n        password: str,\n        force: bool = False\n    ) -> Optional[str]:\n        \"\"\"\n        Authenticate with TradingView.\n        \n        Args:\n            username: TradingView username/email\n            password: TradingView password\n            force: Force re-authentication even if session valid\n            \n        Returns:\n            Authentication token or None if failed\n        \"\"\"\n        with self._lock:\n            if not force and self.is_authenticated:\n                if self._session_data.username == username:\n                    logger.info(\"Using existing valid session\")\n                    return self._session_data.token\n            \n            return self._do_authenticate(username, password)\n    \n    def _do_authenticate(\n        self, \n        username: str, \n        password: str\n    ) -> Optional[str]:\n        \"\"\"Perform actual authentication request.\"\"\"\n        for attempt in range(self._config.max_retries):\n            if not self._rate_limiter.acquire(timeout=30):\n                logger.warning(\"Rate limit exceeded, waiting...\")\n                wait_time = self._rate_limiter.get_wait_time()\n                time.sleep(wait_time)\n                continue\n            \n            try:\n                result = self._send_auth_request(username, password)\n                \n                if result:\n                    self._start_refresh_thread()\n                    return result\n                    \n            except requests.exceptions.RequestException as e:\n                logger.warning(f\"Auth attempt {attempt + 1} failed: {e}\")\n                \n                if attempt < self._config.max_retries - 1:\n                    delay = self._config.retry_delay * (2 ** attempt)\n                    logger.info(f\"Retrying in {delay:.1f}s...\")\n                    time.sleep(delay)\n        \n        logger.error(\"Authentication failed after all retries\")\n        return None\n    \n    def _send_auth_request(\n        self, \n        username: str, \n        password: str\n    ) -> Optional[str]:\n        \"\"\"Send authentication request to TradingView.\"\"\"\n        session = requests.Session()\n        \n        session.cookies.set(\n            \"cookiePrivacyPreferenceBannerProduction\", \n            \"notApplicable\", \n            domain=\".tradingview.com\"\n        )\n        session.cookies.set(\n            \"cookiesSettings\", \n            '{\"analytics\":true,\"advertising\":true}', \n            domain=\".tradingview.com\"\n        )\n        \n        files = {\n            \"username\": (None, username),\n            \"password\": (None, password),\n            \"remember\": (None, \"true\"),\n        }\n        \n        response = session.post(\n            self._config.sign_in_url,\n            files=files,\n            headers=self._HEADERS_TEMPLATE.copy(),\n            timeout=15\n        )\n        response.raise_for_status()\n        \n        result = response.json()\n        \n        if result.get(\"error\"):\n            error_msg = result.get(\"error\", \"Unknown error\")\n            logger.error(f\"Login error: {error_msg}\")\n            return None\n        \n        user_data = result.get(\"user\", {})\n        \n        token = user_data.get(\"auth_token\")\n        if not token:\n            token = session.cookies.get(\"sessionid\")\n        if not token:\n            token = f\"session:{session.cookies.get('sessionid', 'unknown')}\"\n        \n        session_id = session.cookies.get(\"sessionid\", \"\")\n        \n        cookies = {}\n        for cookie in session.cookies:\n            cookies[cookie.name] = cookie.value\n        \n        self._session_data = SessionData(\n            token=token,\n            session_id=session_id,\n            username=user_data.get(\"username\", username),\n            created_at=datetime.now(),\n            expires_at=datetime.now() + timedelta(days=90),\n            cookies=cookies\n        )\n        \n        self._http_session = session\n        self._save_session()\n        \n        logger.info(f\"Login successful for user: {self._session_data.username}\")\n        return token\n    \n    def _start_refresh_thread(self) -> None:\n        \"\"\"Start background token refresh thread.\"\"\"\n        if self._refresh_thread is not None and self._refresh_thread.is_alive():\n            return\n        \n        self._stop_refresh.clear()\n        self._refresh_thread = threading.Thread(\n            target=self._refresh_loop,\n            daemon=True,\n            name=\"auth_refresh\"\n        )\n        self._refresh_thread.start()\n    \n    def _refresh_loop(self) -> None:\n        \"\"\"Background loop to refresh token before expiry.\"\"\"\n        while not self._stop_refresh.is_set():\n            if self._session_data is None:\n                break\n            \n            if self._session_data.is_near_expiry(threshold_minutes=60):\n                logger.info(\"Session near expiry, refreshing...\")\n                break\n            \n            self._stop_refresh.wait(timeout=300)\n    \n    def refresh_session(\n        self, \n        username: str, \n        password: str\n    ) -> Optional[str]:\n        \"\"\"\n        Refresh the current session.\n        \n        Args:\n            username: TradingView username\n            password: TradingView password\n            \n        Returns:\n            New token or None if failed\n        \"\"\"\n        return self.authenticate(username, password, force=True)\n    \n    def logout(self) -> None:\n        \"\"\"Logout and clear session.\"\"\"\n        self._stop_refresh.set()\n        self._clear_session()\n        self._http_session = None\n        logger.info(\"Logged out successfully\")\n    \n    def get_session_info(self) -> Dict[str, Any]:\n        \"\"\"Get current session information.\"\"\"\n        if self._session_data is None:\n            return {\"authenticated\": False}\n        \n        return {\n            \"authenticated\": True,\n            \"username\": self._session_data.username,\n            \"created_at\": self._session_data.created_at.isoformat(),\n            \"expires_at\": self._session_data.expires_at.isoformat(),\n            \"is_expired\": self._session_data.is_expired(),\n            \"is_near_expiry\": self._session_data.is_near_expiry()\n        }\n    \n    def __del__(self):\n        \"\"\"Cleanup on deletion.\"\"\"\n        self._stop_refresh.set()\n","path":null,"size_bytes":14863,"size_tokens":null},"xnoxs_fetcher/cache.py":{"content":"\"\"\"\nXnoxsFetcher Cache Module\n\nThis module provides local caching for historical data to reduce\nAPI calls and improve performance.\n\nAuthor: developerxnoxs\n\"\"\"\n\nfrom __future__ import annotations\n\nimport hashlib\nimport json\nimport logging\nimport os\nimport sqlite3\nimport threading\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import Optional, Dict, Any, List\n\nimport pandas as pd\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass CacheConfig:\n    \"\"\"Configuration for data cache.\"\"\"\n    cache_dir: str = \".tv_cache\"\n    db_name: str = \"historical_data.db\"\n    default_ttl_hours: int = 24\n    max_cache_size_mb: int = 500\n    cleanup_interval_hours: int = 6\n\n\nclass CacheTTL:\n    \"\"\"\n    Preset TTL (Time-To-Live) values for different use cases.\n    \n    Shorter TTL = Data lebih segar, tapi lebih sering request API\n    Longer TTL = Data mungkin tidak terbaru, tapi lebih cepat & hemat bandwidth\n    \"\"\"\n    REALTIME = 0        # Tidak pakai cache sama sekali\n    MINUTES_5 = 5/60    # 5 menit - untuk scalping\n    MINUTES_15 = 15/60  # 15 menit - untuk day trading\n    MINUTES_30 = 30/60  # 30 menit\n    HOUR_1 = 1          # 1 jam - untuk swing trading intraday\n    HOURS_4 = 4         # 4 jam\n    HOURS_12 = 12       # 12 jam\n    DAILY = 24          # 24 jam (default) - untuk analisis harian\n    WEEKLY = 168        # 1 minggu - untuk data historical\n\n\nclass DataCache:\n    \"\"\"\n    Local SQLite cache for historical market data.\n    \n    Features:\n        - Automatic cache expiration\n        - Symbol/exchange/timeframe indexing\n        - Disk space management\n        - Thread-safe operations\n    \n    Example:\n        >>> cache = DataCache()\n        >>> cache.set(\"AAPL\", \"NASDAQ\", \"1D\", df, bars=100)\n        >>> cached_df = cache.get(\"AAPL\", \"NASDAQ\", \"1D\", bars=100)\n    \n    Author: developerxnoxs\n    \"\"\"\n    \n    def __init__(self, config: Optional[CacheConfig] = None):\n        \"\"\"\n        Initialize DataCache.\n        \n        Args:\n            config: Cache configuration\n        \"\"\"\n        self._config = config or CacheConfig()\n        self._lock = threading.Lock()\n        \n        self._cache_dir = Path(self._config.cache_dir)\n        self._cache_dir.mkdir(parents=True, exist_ok=True)\n        \n        self._db_path = self._cache_dir / self._config.db_name\n        self._init_database()\n    \n    def _get_connection(self) -> sqlite3.Connection:\n        \"\"\"Get database connection.\"\"\"\n        conn = sqlite3.connect(\n            str(self._db_path),\n            check_same_thread=False,\n            timeout=30.0\n        )\n        conn.row_factory = sqlite3.Row\n        return conn\n    \n    def _init_database(self) -> None:\n        \"\"\"Initialize database schema.\"\"\"\n        with self._lock:\n            conn = self._get_connection()\n            try:\n                cursor = conn.cursor()\n                \n                cursor.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS cache_entries (\n                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n                        cache_key TEXT UNIQUE NOT NULL,\n                        symbol TEXT NOT NULL,\n                        exchange TEXT NOT NULL,\n                        timeframe TEXT NOT NULL,\n                        bars INTEGER NOT NULL,\n                        data_json TEXT NOT NULL,\n                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                        expires_at TIMESTAMP NOT NULL,\n                        access_count INTEGER DEFAULT 0,\n                        last_accessed TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                    )\n                \"\"\")\n                \n                cursor.execute(\"\"\"\n                    CREATE INDEX IF NOT EXISTS idx_cache_key \n                    ON cache_entries(cache_key)\n                \"\"\")\n                \n                cursor.execute(\"\"\"\n                    CREATE INDEX IF NOT EXISTS idx_symbol_exchange \n                    ON cache_entries(symbol, exchange)\n                \"\"\")\n                \n                cursor.execute(\"\"\"\n                    CREATE INDEX IF NOT EXISTS idx_expires_at \n                    ON cache_entries(expires_at)\n                \"\"\")\n                \n                conn.commit()\n                logger.debug(\"Cache database initialized\")\n                \n            finally:\n                conn.close()\n    \n    def _generate_cache_key(\n        self,\n        symbol: str,\n        exchange: str,\n        timeframe: str,\n        bars: int\n    ) -> str:\n        \"\"\"Generate unique cache key.\"\"\"\n        raw_key = f\"{symbol}:{exchange}:{timeframe}:{bars}\"\n        return hashlib.md5(raw_key.encode()).hexdigest()\n    \n    def _dataframe_to_json(self, df: pd.DataFrame) -> str:\n        \"\"\"Convert DataFrame to JSON string.\"\"\"\n        df_reset = df.reset_index()\n        \n        if 'datetime' in df_reset.columns:\n            df_reset['datetime'] = df_reset['datetime'].astype(str)\n        \n        return df_reset.to_json(orient='records')\n    \n    def _json_to_dataframe(self, json_str: str) -> pd.DataFrame:\n        \"\"\"Convert JSON string back to DataFrame.\"\"\"\n        from io import StringIO\n        df = pd.read_json(StringIO(json_str), orient='records')\n        \n        if 'datetime' in df.columns:\n            df['datetime'] = pd.to_datetime(df['datetime'])\n            df = df.set_index('datetime')\n        \n        return df\n    \n    def get(\n        self,\n        symbol: str,\n        exchange: str,\n        timeframe: str,\n        bars: int\n    ) -> Optional[pd.DataFrame]:\n        \"\"\"\n        Get cached data if available and not expired.\n        \n        Args:\n            symbol: Trading symbol\n            exchange: Exchange name\n            timeframe: Chart timeframe\n            bars: Number of bars\n            \n        Returns:\n            Cached DataFrame or None if not found/expired\n        \"\"\"\n        cache_key = self._generate_cache_key(symbol, exchange, timeframe, bars)\n        \n        with self._lock:\n            conn = self._get_connection()\n            try:\n                cursor = conn.cursor()\n                \n                cursor.execute(\"\"\"\n                    SELECT data_json, expires_at \n                    FROM cache_entries \n                    WHERE cache_key = ?\n                \"\"\", (cache_key,))\n                \n                row = cursor.fetchone()\n                \n                if row is None:\n                    return None\n                \n                expires_at = datetime.fromisoformat(row['expires_at'])\n                if datetime.now() >= expires_at:\n                    cursor.execute(\n                        \"DELETE FROM cache_entries WHERE cache_key = ?\",\n                        (cache_key,)\n                    )\n                    conn.commit()\n                    logger.debug(f\"Cache expired for {symbol}:{exchange}:{timeframe}\")\n                    return None\n                \n                cursor.execute(\"\"\"\n                    UPDATE cache_entries \n                    SET access_count = access_count + 1,\n                        last_accessed = CURRENT_TIMESTAMP\n                    WHERE cache_key = ?\n                \"\"\", (cache_key,))\n                conn.commit()\n                \n                df = self._json_to_dataframe(row['data_json'])\n                logger.debug(f\"Cache hit for {symbol}:{exchange}:{timeframe}\")\n                return df\n                \n            finally:\n                conn.close()\n    \n    def set(\n        self,\n        symbol: str,\n        exchange: str,\n        timeframe: str,\n        bars: int,\n        data: pd.DataFrame,\n        ttl_hours: Optional[int] = None\n    ) -> bool:\n        \"\"\"\n        Store data in cache.\n        \n        Args:\n            symbol: Trading symbol\n            exchange: Exchange name\n            timeframe: Chart timeframe\n            bars: Number of bars\n            data: DataFrame to cache\n            ttl_hours: Time-to-live in hours (default from config)\n            \n        Returns:\n            True if cached successfully\n        \"\"\"\n        if data is None or data.empty:\n            return False\n        \n        cache_key = self._generate_cache_key(symbol, exchange, timeframe, bars)\n        ttl = ttl_hours or self._config.default_ttl_hours\n        expires_at = datetime.now() + timedelta(hours=ttl)\n        \n        data_json = self._dataframe_to_json(data)\n        \n        with self._lock:\n            conn = self._get_connection()\n            try:\n                cursor = conn.cursor()\n                \n                cursor.execute(\"\"\"\n                    INSERT OR REPLACE INTO cache_entries \n                    (cache_key, symbol, exchange, timeframe, bars, \n                     data_json, expires_at, created_at, access_count, last_accessed)\n                    VALUES (?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP, 0, CURRENT_TIMESTAMP)\n                \"\"\", (cache_key, symbol, exchange, timeframe, bars, \n                      data_json, expires_at.isoformat()))\n                \n                conn.commit()\n                logger.debug(f\"Cached data for {symbol}:{exchange}:{timeframe}\")\n                return True\n                \n            except sqlite3.Error as e:\n                logger.error(f\"Failed to cache data: {e}\")\n                return False\n            finally:\n                conn.close()\n    \n    def invalidate(\n        self,\n        symbol: Optional[str] = None,\n        exchange: Optional[str] = None,\n        timeframe: Optional[str] = None\n    ) -> int:\n        \"\"\"\n        Invalidate (delete) cached entries.\n        \n        Args:\n            symbol: Filter by symbol (optional)\n            exchange: Filter by exchange (optional)\n            timeframe: Filter by timeframe (optional)\n            \n        Returns:\n            Number of entries deleted\n        \"\"\"\n        conditions = []\n        params = []\n        \n        if symbol:\n            conditions.append(\"symbol = ?\")\n            params.append(symbol)\n        if exchange:\n            conditions.append(\"exchange = ?\")\n            params.append(exchange)\n        if timeframe:\n            conditions.append(\"timeframe = ?\")\n            params.append(timeframe)\n        \n        where_clause = \" AND \".join(conditions) if conditions else \"1=1\"\n        \n        with self._lock:\n            conn = self._get_connection()\n            try:\n                cursor = conn.cursor()\n                cursor.execute(\n                    f\"DELETE FROM cache_entries WHERE {where_clause}\",\n                    params\n                )\n                deleted = cursor.rowcount\n                conn.commit()\n                logger.info(f\"Invalidated {deleted} cache entries\")\n                return deleted\n            finally:\n                conn.close()\n    \n    def cleanup_expired(self) -> int:\n        \"\"\"\n        Remove all expired entries.\n        \n        Returns:\n            Number of entries removed\n        \"\"\"\n        with self._lock:\n            conn = self._get_connection()\n            try:\n                cursor = conn.cursor()\n                cursor.execute(\"\"\"\n                    DELETE FROM cache_entries \n                    WHERE expires_at < CURRENT_TIMESTAMP\n                \"\"\")\n                deleted = cursor.rowcount\n                conn.commit()\n                \n                if deleted > 0:\n                    cursor.execute(\"VACUUM\")\n                    logger.info(f\"Cleaned up {deleted} expired cache entries\")\n                \n                return deleted\n            finally:\n                conn.close()\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Get cache statistics.\n        \n        Returns:\n            Dictionary with cache stats\n        \"\"\"\n        with self._lock:\n            conn = self._get_connection()\n            try:\n                cursor = conn.cursor()\n                \n                cursor.execute(\"SELECT COUNT(*) as count FROM cache_entries\")\n                total_entries = cursor.fetchone()['count']\n                \n                cursor.execute(\"\"\"\n                    SELECT COUNT(*) as count FROM cache_entries \n                    WHERE expires_at < CURRENT_TIMESTAMP\n                \"\"\")\n                expired_entries = cursor.fetchone()['count']\n                \n                cursor.execute(\"SELECT SUM(access_count) as total FROM cache_entries\")\n                row = cursor.fetchone()\n                total_hits = row['total'] or 0\n                \n                db_size = os.path.getsize(self._db_path) / (1024 * 1024)\n                \n                return {\n                    \"total_entries\": total_entries,\n                    \"expired_entries\": expired_entries,\n                    \"valid_entries\": total_entries - expired_entries,\n                    \"total_cache_hits\": total_hits,\n                    \"database_size_mb\": round(db_size, 2),\n                    \"cache_directory\": str(self._cache_dir)\n                }\n            finally:\n                conn.close()\n    \n    def list_cached_symbols(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        List all cached symbol/exchange combinations.\n        \n        Returns:\n            List of cached symbol info\n        \"\"\"\n        with self._lock:\n            conn = self._get_connection()\n            try:\n                cursor = conn.cursor()\n                cursor.execute(\"\"\"\n                    SELECT DISTINCT symbol, exchange, timeframe, \n                           MAX(bars) as max_bars,\n                           MAX(created_at) as last_updated\n                    FROM cache_entries\n                    WHERE expires_at > CURRENT_TIMESTAMP\n                    GROUP BY symbol, exchange, timeframe\n                    ORDER BY symbol, exchange\n                \"\"\")\n                \n                results = []\n                for row in cursor.fetchall():\n                    results.append({\n                        \"symbol\": row['symbol'],\n                        \"exchange\": row['exchange'],\n                        \"timeframe\": row['timeframe'],\n                        \"max_bars\": row['max_bars'],\n                        \"last_updated\": row['last_updated']\n                    })\n                \n                return results\n            finally:\n                conn.close()\n    \n    def get_cache_age(\n        self,\n        symbol: str,\n        exchange: str,\n        timeframe: str,\n        bars: int\n    ) -> Optional[Dict[str, Any]]:\n        \"\"\"\n        Get the age of cached data.\n        \n        Args:\n            symbol: Trading symbol\n            exchange: Exchange name\n            timeframe: Chart timeframe\n            bars: Number of bars\n            \n        Returns:\n            Dictionary with age info or None if not cached:\n            - created_at: When data was cached\n            - age_minutes: How old the cache is in minutes\n            - age_hours: How old the cache is in hours\n            - expires_at: When cache will expire\n            - is_expired: Whether cache is expired\n            - is_fresh: Whether cache is considered fresh (< 1 hour old)\n        \"\"\"\n        cache_key = self._generate_cache_key(symbol, exchange, timeframe, bars)\n        \n        with self._lock:\n            conn = self._get_connection()\n            try:\n                cursor = conn.cursor()\n                cursor.execute(\"\"\"\n                    SELECT created_at, expires_at \n                    FROM cache_entries \n                    WHERE cache_key = ?\n                \"\"\", (cache_key,))\n                \n                row = cursor.fetchone()\n                if row is None:\n                    return None\n                \n                created_at = datetime.fromisoformat(row['created_at'])\n                expires_at = datetime.fromisoformat(row['expires_at'])\n                now = datetime.now()\n                \n                age = now - created_at\n                age_minutes = age.total_seconds() / 60\n                age_hours = age_minutes / 60\n                \n                return {\n                    \"created_at\": created_at.isoformat(),\n                    \"age_minutes\": round(age_minutes, 1),\n                    \"age_hours\": round(age_hours, 2),\n                    \"expires_at\": expires_at.isoformat(),\n                    \"is_expired\": now >= expires_at,\n                    \"is_fresh\": age_minutes < 60  # Fresh if less than 1 hour old\n                }\n            finally:\n                conn.close()\n    \n    def is_fresh(\n        self,\n        symbol: str,\n        exchange: str,\n        timeframe: str,\n        bars: int,\n        max_age_minutes: int = 60\n    ) -> bool:\n        \"\"\"\n        Check if cached data is fresh enough.\n        \n        Args:\n            symbol: Trading symbol\n            exchange: Exchange name\n            timeframe: Chart timeframe\n            bars: Number of bars\n            max_age_minutes: Maximum acceptable age in minutes (default 60)\n            \n        Returns:\n            True if cache exists and is younger than max_age_minutes\n        \"\"\"\n        age_info = self.get_cache_age(symbol, exchange, timeframe, bars)\n        \n        if age_info is None:\n            return False\n        \n        if age_info['is_expired']:\n            return False\n        \n        return age_info['age_minutes'] < max_age_minutes\n    \n    def clear_all(self) -> int:\n        \"\"\"\n        Clear all cache entries.\n        \n        Returns:\n            Number of entries cleared\n        \"\"\"\n        with self._lock:\n            conn = self._get_connection()\n            try:\n                cursor = conn.cursor()\n                cursor.execute(\"SELECT COUNT(*) as count FROM cache_entries\")\n                count = cursor.fetchone()['count']\n                \n                cursor.execute(\"DELETE FROM cache_entries\")\n                conn.commit()\n                \n                cursor.execute(\"VACUUM\")\n                \n                logger.info(f\"Cleared {count} cache entries\")\n                return count\n            finally:\n                conn.close()\n","path":null,"size_bytes":18080,"size_tokens":null},"xnoxs_fetcher/export.py":{"content":"\"\"\"\nXnoxsFetcher Export Module\n\nThis module provides data export functionality to various formats\nincluding CSV, Excel, and JSON.\n\nAuthor: developerxnoxs\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Optional, List, Dict, Any, Union\n\nimport pandas as pd\n\nlogger = logging.getLogger(__name__)\n\n\nclass DataExporter:\n    \"\"\"\n    Export market data to various file formats.\n    \n    Supported formats:\n        - CSV (Comma Separated Values)\n        - Excel (.xlsx)\n        - JSON\n        - Parquet (for large datasets)\n    \n    Example:\n        >>> exporter = DataExporter()\n        >>> exporter.to_csv(df, \"AAPL_daily.csv\")\n        >>> exporter.to_excel(df, \"portfolio.xlsx\", sheet_name=\"AAPL\")\n    \n    Author: developerxnoxs\n    \"\"\"\n    \n    def __init__(self, output_dir: str = \"exports\"):\n        \"\"\"\n        Initialize DataExporter.\n        \n        Args:\n            output_dir: Default directory for exported files\n        \"\"\"\n        self._output_dir = Path(output_dir)\n        self._output_dir.mkdir(parents=True, exist_ok=True)\n    \n    def _prepare_dataframe(\n        self, \n        df: pd.DataFrame,\n        include_symbol: bool = True\n    ) -> pd.DataFrame:\n        \"\"\"Prepare DataFrame for export.\"\"\"\n        export_df = df.copy()\n        \n        if export_df.index.name == 'datetime' or isinstance(export_df.index, pd.DatetimeIndex):\n            export_df = export_df.reset_index()\n        \n        if 'datetime' in export_df.columns:\n            export_df['datetime'] = export_df['datetime'].astype(str)\n        \n        if not include_symbol and 'symbol' in export_df.columns:\n            export_df = export_df.drop(columns=['symbol'])\n        \n        return export_df\n    \n    def _get_filepath(\n        self, \n        filename: str, \n        extension: str\n    ) -> Path:\n        \"\"\"Get full filepath with extension.\"\"\"\n        if not filename.endswith(extension):\n            filename = f\"{filename}{extension}\"\n        \n        if \"/\" in filename or \"\\\\\" in filename:\n            return Path(filename)\n        \n        return self._output_dir / filename\n    \n    def to_csv(\n        self,\n        data: Union[pd.DataFrame, List[pd.DataFrame]],\n        filename: str,\n        include_symbol: bool = True,\n        include_header: bool = True,\n        separator: str = \",\",\n        decimal: str = \".\"\n    ) -> str:\n        \"\"\"\n        Export data to CSV file.\n        \n        Args:\n            data: DataFrame or list of DataFrames to export\n            filename: Output filename\n            include_symbol: Include symbol column\n            include_header: Include column headers\n            separator: Field separator\n            decimal: Decimal point character\n            \n        Returns:\n            Full path to exported file\n        \"\"\"\n        filepath = self._get_filepath(filename, \".csv\")\n        \n        if isinstance(data, list):\n            combined_df = pd.concat(data, ignore_index=True)\n        else:\n            combined_df = data\n        \n        export_df = self._prepare_dataframe(combined_df, include_symbol)\n        \n        export_df.to_csv(\n            filepath,\n            index=False,\n            header=include_header,\n            sep=separator,\n            decimal=decimal\n        )\n        \n        logger.info(f\"Exported {len(export_df)} rows to {filepath}\")\n        return str(filepath)\n    \n    def to_excel(\n        self,\n        data: Union[pd.DataFrame, Dict[str, pd.DataFrame]],\n        filename: str,\n        sheet_name: str = \"Data\",\n        include_symbol: bool = True,\n        auto_column_width: bool = True\n    ) -> str:\n        \"\"\"\n        Export data to Excel file.\n        \n        Args:\n            data: DataFrame or dict of sheet_name->DataFrame\n            filename: Output filename\n            sheet_name: Sheet name (if single DataFrame)\n            include_symbol: Include symbol column\n            auto_column_width: Auto-adjust column widths\n            \n        Returns:\n            Full path to exported file\n        \"\"\"\n        filepath = self._get_filepath(filename, \".xlsx\")\n        \n        if isinstance(data, dict):\n            sheets = {\n                name: self._prepare_dataframe(df, include_symbol) \n                for name, df in data.items()\n            }\n        else:\n            sheets = {sheet_name: self._prepare_dataframe(data, include_symbol)}\n        \n        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n            for name, df in sheets.items():\n                df.to_excel(writer, sheet_name=name, index=False)\n                \n                if auto_column_width:\n                    worksheet = writer.sheets[name]\n                    for idx, col in enumerate(df.columns):\n                        max_length = max(\n                            df[col].astype(str).map(len).max(),\n                            len(str(col))\n                        ) + 2\n                        worksheet.column_dimensions[\n                            chr(65 + idx)\n                        ].width = min(max_length, 50)\n        \n        logger.info(f\"Exported to Excel: {filepath}\")\n        return str(filepath)\n    \n    def to_json(\n        self,\n        data: pd.DataFrame,\n        filename: str,\n        orient: str = \"records\",\n        indent: int = 2,\n        include_metadata: bool = True\n    ) -> str:\n        \"\"\"\n        Export data to JSON file.\n        \n        Args:\n            data: DataFrame to export\n            filename: Output filename\n            orient: JSON orientation (records, columns, index, etc.)\n            indent: JSON indentation\n            include_metadata: Include export metadata\n            \n        Returns:\n            Full path to exported file\n        \"\"\"\n        filepath = self._get_filepath(filename, \".json\")\n        \n        export_df = self._prepare_dataframe(data)\n        \n        if include_metadata:\n            output = {\n                \"metadata\": {\n                    \"exported_at\": datetime.now().isoformat(),\n                    \"total_records\": len(export_df),\n                    \"columns\": list(export_df.columns)\n                },\n                \"data\": json.loads(export_df.to_json(orient=orient))\n            }\n            \n            with open(filepath, 'w') as f:\n                json.dump(output, f, indent=indent)\n        else:\n            export_df.to_json(filepath, orient=orient, indent=indent)\n        \n        logger.info(f\"Exported {len(export_df)} rows to JSON: {filepath}\")\n        return str(filepath)\n    \n    def to_parquet(\n        self,\n        data: pd.DataFrame,\n        filename: str,\n        compression: str = \"snappy\"\n    ) -> str:\n        \"\"\"\n        Export data to Parquet file (efficient for large datasets).\n        \n        Args:\n            data: DataFrame to export\n            filename: Output filename\n            compression: Compression algorithm (snappy, gzip, brotli)\n            \n        Returns:\n            Full path to exported file\n        \"\"\"\n        filepath = self._get_filepath(filename, \".parquet\")\n        \n        data.to_parquet(filepath, compression=compression, index=True)\n        \n        logger.info(f\"Exported to Parquet: {filepath}\")\n        return str(filepath)\n    \n    def export_multiple(\n        self,\n        data_dict: Dict[str, pd.DataFrame],\n        base_filename: str,\n        format: str = \"csv\"\n    ) -> List[str]:\n        \"\"\"\n        Export multiple DataFrames to separate files.\n        \n        Args:\n            data_dict: Dictionary of name->DataFrame\n            base_filename: Base filename (symbol will be appended)\n            format: Export format (csv, excel, json)\n            \n        Returns:\n            List of exported file paths\n        \"\"\"\n        exported_files = []\n        \n        for name, df in data_dict.items():\n            filename = f\"{base_filename}_{name}\"\n            \n            if format == \"csv\":\n                path = self.to_csv(df, filename)\n            elif format == \"excel\":\n                path = self.to_excel(df, filename)\n            elif format == \"json\":\n                path = self.to_json(df, filename)\n            else:\n                raise ValueError(f\"Unsupported format: {format}\")\n            \n            exported_files.append(path)\n        \n        return exported_files\n    \n    def create_summary_report(\n        self,\n        data: pd.DataFrame,\n        filename: str = \"summary_report\"\n    ) -> str:\n        \"\"\"\n        Create a summary report with statistics.\n        \n        Args:\n            data: DataFrame to analyze\n            filename: Output filename\n            \n        Returns:\n            Path to report file\n        \"\"\"\n        filepath = self._get_filepath(filename, \".txt\")\n        \n        report_lines = [\n            \"=\" * 60,\n            \"MARKET DATA SUMMARY REPORT\",\n            f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n            \"=\" * 60,\n            \"\",\n            \"DATA OVERVIEW\",\n            \"-\" * 40,\n            f\"Total Records: {len(data)}\",\n            f\"Date Range: {data.index.min()} to {data.index.max()}\",\n            \"\",\n        ]\n        \n        if 'symbol' in data.columns:\n            symbols = data['symbol'].unique()\n            report_lines.append(f\"Symbols: {', '.join(symbols)}\")\n            report_lines.append(\"\")\n        \n        numeric_cols = ['open', 'high', 'low', 'close', 'volume']\n        available_cols = [c for c in numeric_cols if c in data.columns]\n        \n        if available_cols:\n            report_lines.append(\"PRICE STATISTICS\")\n            report_lines.append(\"-\" * 40)\n            \n            for col in available_cols:\n                if col == 'volume':\n                    report_lines.append(\n                        f\"Total Volume: {data[col].sum():,.0f}\"\n                    )\n                    report_lines.append(\n                        f\"Avg Volume: {data[col].mean():,.0f}\"\n                    )\n                else:\n                    report_lines.append(\n                        f\"{col.upper()}: Min={data[col].min():.4f}, \"\n                        f\"Max={data[col].max():.4f}, \"\n                        f\"Mean={data[col].mean():.4f}\"\n                    )\n            \n            report_lines.append(\"\")\n            \n            if 'close' in data.columns and len(data) > 1:\n                report_lines.append(\"PERFORMANCE\")\n                report_lines.append(\"-\" * 40)\n                \n                first_close = data['close'].iloc[0]\n                last_close = data['close'].iloc[-1]\n                pct_change = ((last_close - first_close) / first_close) * 100\n                \n                report_lines.append(f\"First Close: {first_close:.4f}\")\n                report_lines.append(f\"Last Close: {last_close:.4f}\")\n                report_lines.append(f\"Change: {pct_change:+.2f}%\")\n        \n        report_lines.append(\"\")\n        report_lines.append(\"=\" * 60)\n        \n        with open(filepath, 'w') as f:\n            f.write('\\n'.join(report_lines))\n        \n        logger.info(f\"Created summary report: {filepath}\")\n        return str(filepath)\n\n\ndef quick_export(\n    data: pd.DataFrame,\n    filename: str,\n    format: str = \"csv\"\n) -> str:\n    \"\"\"\n    Quick export function for simple use cases.\n    \n    Args:\n        data: DataFrame to export\n        filename: Output filename\n        format: Export format (csv, excel, json, parquet)\n        \n    Returns:\n        Path to exported file\n    \"\"\"\n    exporter = DataExporter()\n    \n    format_map = {\n        \"csv\": exporter.to_csv,\n        \"excel\": exporter.to_excel,\n        \"json\": exporter.to_json,\n        \"parquet\": exporter.to_parquet\n    }\n    \n    if format not in format_map:\n        raise ValueError(f\"Unsupported format: {format}\")\n    \n    return format_map[format](data, filename)\n","path":null,"size_bytes":11874,"size_tokens":null},"xnoxs_fetcher/websocket_manager.py":{"content":"\"\"\"\nXnoxsFetcher WebSocket Manager Module\n\nThis module provides robust WebSocket connection management with\nautomatic reconnection, heartbeat, and connection state tracking.\n\nAuthor: developerxnoxs\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport threading\nimport time\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Optional, Callable, Any, List\n\nfrom websocket import create_connection, WebSocket, WebSocketException\n\nlogger = logging.getLogger(__name__)\n\n\nclass ConnectionState(Enum):\n    \"\"\"WebSocket connection states.\"\"\"\n    DISCONNECTED = \"disconnected\"\n    CONNECTING = \"connecting\"\n    CONNECTED = \"connected\"\n    RECONNECTING = \"reconnecting\"\n    CLOSED = \"closed\"\n\n\n@dataclass\nclass WebSocketConfig:\n    \"\"\"Configuration for WebSocket manager.\"\"\"\n    endpoint: str = \"wss://data.tradingview.com/socket.io/websocket\"\n    origin: str = \"https://data.tradingview.com\"\n    timeout: int = 5\n    heartbeat_interval: int = 10\n    max_reconnect_attempts: int = 5\n    reconnect_delay: float = 1.0\n    reconnect_delay_max: float = 30.0\n    ping_timeout: int = 30\n\n\nclass WebSocketManager:\n    \"\"\"\n    Robust WebSocket Connection Manager.\n    \n    Features:\n        - Automatic reconnection with exponential backoff\n        - Heartbeat/ping mechanism\n        - Connection state tracking\n        - Thread-safe operations\n        - Event callbacks for state changes\n    \n    Example:\n        >>> ws_manager = WebSocketManager()\n        >>> ws_manager.connect()\n        >>> ws_manager.send_message(\"set_auth_token\", [\"token\"])\n        >>> response = ws_manager.receive()\n    \n    Author: developerxnoxs\n    \"\"\"\n    \n    def __init__(\n        self, \n        config: Optional[WebSocketConfig] = None,\n        on_state_change: Optional[Callable[[ConnectionState], None]] = None,\n        on_message: Optional[Callable[[str], None]] = None,\n        on_error: Optional[Callable[[Exception], None]] = None\n    ):\n        \"\"\"\n        Initialize WebSocketManager.\n        \n        Args:\n            config: WebSocket configuration\n            on_state_change: Callback for connection state changes\n            on_message: Callback for received messages\n            on_error: Callback for errors\n        \"\"\"\n        self._config = config or WebSocketConfig()\n        self._ws: Optional[WebSocket] = None\n        self._state = ConnectionState.DISCONNECTED\n        self._lock = threading.Lock()\n        self._reconnect_count = 0\n        self._last_pong = time.time()\n        \n        self._on_state_change = on_state_change\n        self._on_message = on_message\n        self._on_error = on_error\n        \n        self._heartbeat_thread: Optional[threading.Thread] = None\n        self._stop_heartbeat = threading.Event()\n        \n        self._message_buffer: List[str] = []\n    \n    @property\n    def state(self) -> ConnectionState:\n        \"\"\"Get current connection state.\"\"\"\n        return self._state\n    \n    @property\n    def is_connected(self) -> bool:\n        \"\"\"Check if currently connected.\"\"\"\n        return self._state == ConnectionState.CONNECTED\n    \n    def _set_state(self, new_state: ConnectionState) -> None:\n        \"\"\"Update connection state and notify callback.\"\"\"\n        old_state = self._state\n        self._state = new_state\n        \n        if old_state != new_state:\n            logger.info(f\"WebSocket state: {old_state.value} -> {new_state.value}\")\n            if self._on_state_change:\n                try:\n                    self._on_state_change(new_state)\n                except Exception as e:\n                    logger.error(f\"State change callback error: {e}\")\n    \n    def connect(self) -> bool:\n        \"\"\"\n        Establish WebSocket connection.\n        \n        Returns:\n            True if connected successfully\n        \"\"\"\n        with self._lock:\n            if self._state == ConnectionState.CONNECTED:\n                return True\n            \n            self._set_state(ConnectionState.CONNECTING)\n            \n            try:\n                self._ws = create_connection(\n                    self._config.endpoint,\n                    origin=self._config.origin,\n                    timeout=self._config.timeout\n                )\n                \n                self._set_state(ConnectionState.CONNECTED)\n                self._reconnect_count = 0\n                self._last_pong = time.time()\n                \n                self._start_heartbeat()\n                \n                logger.info(\"WebSocket connected successfully\")\n                return True\n                \n            except Exception as e:\n                logger.error(f\"WebSocket connection failed: {e}\")\n                self._set_state(ConnectionState.DISCONNECTED)\n                \n                if self._on_error:\n                    self._on_error(e)\n                \n                return False\n    \n    def disconnect(self) -> None:\n        \"\"\"Close WebSocket connection.\"\"\"\n        with self._lock:\n            self._stop_heartbeat.set()\n            \n            if self._ws:\n                try:\n                    self._ws.close()\n                except Exception:\n                    pass\n                self._ws = None\n            \n            self._set_state(ConnectionState.CLOSED)\n            logger.info(\"WebSocket disconnected\")\n    \n    def reconnect(self) -> bool:\n        \"\"\"\n        Attempt to reconnect with exponential backoff.\n        \n        Returns:\n            True if reconnected successfully\n        \"\"\"\n        if self._state == ConnectionState.CLOSED:\n            logger.warning(\"Cannot reconnect - connection was explicitly closed\")\n            return False\n        \n        self._set_state(ConnectionState.RECONNECTING)\n        \n        for attempt in range(self._config.max_reconnect_attempts):\n            self._reconnect_count = attempt + 1\n            \n            delay = min(\n                self._config.reconnect_delay * (2 ** attempt),\n                self._config.reconnect_delay_max\n            )\n            \n            logger.info(\n                f\"Reconnect attempt {self._reconnect_count}/\"\n                f\"{self._config.max_reconnect_attempts} in {delay:.1f}s\"\n            )\n            \n            time.sleep(delay)\n            \n            if self._ws:\n                try:\n                    self._ws.close()\n                except Exception:\n                    pass\n                self._ws = None\n            \n            if self.connect():\n                logger.info(\"Reconnected successfully\")\n                return True\n        \n        logger.error(\"Failed to reconnect after all attempts\")\n        self._set_state(ConnectionState.DISCONNECTED)\n        return False\n    \n    def _start_heartbeat(self) -> None:\n        \"\"\"Start heartbeat thread.\"\"\"\n        if self._heartbeat_thread and self._heartbeat_thread.is_alive():\n            return\n        \n        self._stop_heartbeat.clear()\n        self._heartbeat_thread = threading.Thread(\n            target=self._heartbeat_loop,\n            daemon=True,\n            name=\"ws_heartbeat\"\n        )\n        self._heartbeat_thread.start()\n    \n    def _heartbeat_loop(self) -> None:\n        \"\"\"Heartbeat loop to keep connection alive.\"\"\"\n        while not self._stop_heartbeat.is_set():\n            if self._state != ConnectionState.CONNECTED:\n                break\n            \n            if time.time() - self._last_pong > self._config.ping_timeout:\n                logger.warning(\"Ping timeout - attempting reconnect\")\n                self.reconnect()\n                break\n            \n            try:\n                if self._ws:\n                    self._ws.ping()\n            except Exception as e:\n                logger.warning(f\"Ping failed: {e}\")\n            \n            self._stop_heartbeat.wait(timeout=self._config.heartbeat_interval)\n    \n    def send(self, message: str) -> bool:\n        \"\"\"\n        Send raw message through WebSocket.\n        \n        Args:\n            message: Message string to send\n            \n        Returns:\n            True if sent successfully\n        \"\"\"\n        if not self.is_connected or self._ws is None:\n            if not self.reconnect():\n                return False\n        \n        try:\n            self._ws.send(message)\n            return True\n        except WebSocketException as e:\n            logger.error(f\"Send failed: {e}\")\n            if self._on_error:\n                self._on_error(e)\n            return False\n    \n    def send_message(self, func_name: str, params: List[Any]) -> bool:\n        \"\"\"\n        Send formatted TradingView message.\n        \n        Args:\n            func_name: Function name\n            params: Parameters list\n            \n        Returns:\n            True if sent successfully\n        \"\"\"\n        message = self._build_message(func_name, params)\n        formatted = self._add_header(message)\n        return self.send(formatted)\n    \n    @staticmethod\n    def _add_header(message: str) -> str:\n        \"\"\"Add TradingView message header.\"\"\"\n        return f\"~m~{len(message)}~m~{message}\"\n    \n    @staticmethod\n    def _build_message(func_name: str, params: List[Any]) -> str:\n        \"\"\"Build JSON message for WebSocket.\"\"\"\n        return json.dumps({\"m\": func_name, \"p\": params}, separators=(\",\", \":\"))\n    \n    def receive(self, timeout: Optional[float] = None) -> Optional[str]:\n        \"\"\"\n        Receive message from WebSocket.\n        \n        Args:\n            timeout: Receive timeout in seconds\n            \n        Returns:\n            Received message or None\n        \"\"\"\n        if not self.is_connected or self._ws is None:\n            if not self.reconnect():\n                return None\n        \n        try:\n            if timeout:\n                self._ws.settimeout(timeout)\n            \n            result = self._ws.recv()\n            self._last_pong = time.time()\n            \n            if self._on_message:\n                self._on_message(result)\n            \n            return result\n            \n        except WebSocketException as e:\n            logger.error(f\"Receive failed: {e}\")\n            if self._on_error:\n                self._on_error(e)\n            return None\n    \n    def receive_until(\n        self, \n        stop_condition: str,\n        timeout: float = 60.0\n    ) -> str:\n        \"\"\"\n        Receive messages until stop condition is met.\n        \n        Args:\n            stop_condition: String to look for in messages\n            timeout: Maximum wait time\n            \n        Returns:\n            Concatenated received messages\n        \"\"\"\n        raw_data = \"\"\n        start_time = time.time()\n        \n        while True:\n            if time.time() - start_time > timeout:\n                logger.warning(\"Receive timeout reached\")\n                break\n            \n            try:\n                result = self.receive(timeout=5.0)\n                if result:\n                    raw_data += result + \"\\n\"\n                    \n                    if stop_condition in result:\n                        break\n            except Exception as e:\n                logger.error(f\"Receive error: {e}\")\n                break\n        \n        return raw_data\n    \n    def get_stats(self) -> dict:\n        \"\"\"Get connection statistics.\"\"\"\n        return {\n            \"state\": self._state.value,\n            \"reconnect_count\": self._reconnect_count,\n            \"last_pong_seconds_ago\": time.time() - self._last_pong,\n            \"is_connected\": self.is_connected\n        }\n\n\nclass WebSocketPool:\n    \"\"\"\n    Pool of WebSocket connections for parallel operations.\n    \n    Manages multiple WebSocket connections for concurrent\n    data fetching operations.\n    \"\"\"\n    \n    def __init__(\n        self, \n        pool_size: int = 3,\n        config: Optional[WebSocketConfig] = None\n    ):\n        \"\"\"\n        Initialize WebSocket pool.\n        \n        Args:\n            pool_size: Number of connections in pool\n            config: WebSocket configuration\n        \"\"\"\n        self._pool_size = pool_size\n        self._config = config or WebSocketConfig()\n        self._connections: List[WebSocketManager] = []\n        self._available: List[WebSocketManager] = []\n        self._lock = threading.Lock()\n        self._condition = threading.Condition(self._lock)\n    \n    def initialize(self) -> None:\n        \"\"\"Initialize all connections in pool.\"\"\"\n        for _ in range(self._pool_size):\n            ws = WebSocketManager(config=self._config)\n            ws.connect()\n            self._connections.append(ws)\n            self._available.append(ws)\n    \n    def acquire(self, timeout: float = 30.0) -> Optional[WebSocketManager]:\n        \"\"\"\n        Acquire a connection from the pool.\n        \n        Args:\n            timeout: Maximum wait time\n            \n        Returns:\n            WebSocketManager or None if timeout\n        \"\"\"\n        with self._condition:\n            start = time.time()\n            \n            while not self._available:\n                remaining = timeout - (time.time() - start)\n                if remaining <= 0:\n                    return None\n                self._condition.wait(timeout=remaining)\n            \n            return self._available.pop()\n    \n    def release(self, ws: WebSocketManager) -> None:\n        \"\"\"\n        Release connection back to pool.\n        \n        Args:\n            ws: WebSocketManager to release\n        \"\"\"\n        with self._condition:\n            if ws in self._connections:\n                self._available.append(ws)\n                self._condition.notify()\n    \n    def shutdown(self) -> None:\n        \"\"\"Close all connections in pool.\"\"\"\n        with self._lock:\n            for ws in self._connections:\n                ws.disconnect()\n            self._connections.clear()\n            self._available.clear()\n","path":null,"size_bytes":13804,"size_tokens":null},"demo_features.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nXnoxsFetcher New Features Demo\n\nDemonstrates the new features:\n1. Auth Manager - Session management\n2. Cache - Local data caching\n3. Export - CSV/Excel/JSON export\n4. Parallel - Multi-symbol fetching\n\nAuthor: developerxnoxs\n\"\"\"\n\nimport os\nimport sys\nimport time\n\nfrom xnoxs_fetcher import (\n    XnoxsFetcher,\n    TimeFrame,\n    AuthManager,\n    DataCache,\n    DataExporter,\n    ParallelFetcher,\n    fetch_parallel,\n)\n\n\ndef print_header(title: str) -> None:\n    \"\"\"Print formatted section header.\"\"\"\n    line = \"=\" * 60\n    print(f\"\\n{line}\")\n    print(f\"  {title}\")\n    print(f\"{line}\\n\")\n\n\ndef demo_auth_manager():\n    \"\"\"Demo: Auth Manager with session persistence.\"\"\"\n    print_header(\"1. AUTH MANAGER DEMO\")\n    \n    username = os.environ.get(\"TRADINGVIEW_USERNAME\")\n    password = os.environ.get(\"TRADINGVIEW_PASSWORD\")\n    \n    if not username or not password:\n        print(\"  [SKIP] No credentials found in environment\")\n        print(\"  Set TRADINGVIEW_USERNAME and TRADINGVIEW_PASSWORD to test\")\n        return None\n    \n    print(\"  Initializing Auth Manager...\")\n    auth = AuthManager()\n    \n    print(f\"  Session info: {auth.get_session_info()}\")\n    \n    print(\"  Authenticating...\")\n    token = auth.authenticate(username, password)\n    \n    if token:\n        print(f\"  [OK] Login successful!\")\n        print(f\"  Username: {auth.username}\")\n        print(f\"  Token: {token[:30]}...\")\n        print(f\"  Session info: {auth.get_session_info()}\")\n    else:\n        print(\"  [FAILED] Login failed\")\n    \n    return auth\n\n\ndef demo_cache():\n    \"\"\"Demo: Data caching.\"\"\"\n    print_header(\"2. CACHE DEMO\")\n    \n    print(\"  Initializing cache...\")\n    cache = DataCache()\n    \n    print(f\"  Cache stats: {cache.get_stats()}\")\n    \n    fetcher = XnoxsFetcher()\n    \n    print(\"\\n  Fetching AAPL data (first time - from API)...\")\n    start = time.time()\n    data = fetcher.get_historical_data(\"AAPL\", \"NASDAQ\", TimeFrame.DAILY, 50)\n    api_time = time.time() - start\n    print(f\"  API fetch time: {api_time:.2f}s\")\n    \n    if data is not None:\n        print(f\"  Got {len(data)} rows\")\n        \n        print(\"\\n  Storing in cache...\")\n        cache.set(\"AAPL\", \"NASDAQ\", \"1D\", 50, data)\n        \n        print(\"\\n  Fetching from cache...\")\n        start = time.time()\n        cached_data = cache.get(\"AAPL\", \"NASDAQ\", \"1D\", 50)\n        cache_time = time.time() - start\n        print(f\"  Cache fetch time: {cache_time:.4f}s\")\n        \n        if cached_data is not None:\n            print(f\"  [OK] Cache hit! Got {len(cached_data)} rows\")\n            print(f\"  Speed improvement: {api_time / cache_time:.0f}x faster\")\n    \n    print(f\"\\n  Cached symbols: {cache.list_cached_symbols()}\")\n    print(f\"  Final cache stats: {cache.get_stats()}\")\n    \n    return cache\n\n\ndef demo_export(data=None):\n    \"\"\"Demo: Data export.\"\"\"\n    print_header(\"3. EXPORT DEMO\")\n    \n    if data is None:\n        print(\"  Fetching sample data...\")\n        fetcher = XnoxsFetcher()\n        data = fetcher.get_historical_data(\"AAPL\", \"NASDAQ\", TimeFrame.DAILY, 20)\n    \n    if data is None:\n        print(\"  [SKIP] No data available for export\")\n        return\n    \n    print(f\"  Data to export: {len(data)} rows\")\n    \n    exporter = DataExporter(output_dir=\"exports\")\n    \n    print(\"\\n  Exporting to CSV...\")\n    csv_path = exporter.to_csv(data, \"AAPL_daily\")\n    print(f\"  [OK] Exported: {csv_path}\")\n    \n    print(\"\\n  Exporting to JSON...\")\n    json_path = exporter.to_json(data, \"AAPL_daily\")\n    print(f\"  [OK] Exported: {json_path}\")\n    \n    print(\"\\n  Exporting to Excel...\")\n    try:\n        excel_path = exporter.to_excel(data, \"AAPL_daily\", sheet_name=\"Daily Data\")\n        print(f\"  [OK] Exported: {excel_path}\")\n    except Exception as e:\n        print(f\"  [WARN] Excel export issue: {e}\")\n    \n    print(\"\\n  Creating summary report...\")\n    report_path = exporter.create_summary_report(data, \"AAPL_report\")\n    print(f\"  [OK] Report: {report_path}\")\n    \n    with open(report_path, 'r') as f:\n        print(\"\\n  Report preview:\")\n        for line in f.readlines()[:15]:\n            print(f\"    {line.rstrip()}\")\n\n\ndef demo_parallel():\n    \"\"\"Demo: Parallel fetching.\"\"\"\n    print_header(\"4. PARALLEL FETCH DEMO\")\n    \n    fetcher = XnoxsFetcher()\n    \n    symbols = [\n        (\"AAPL\", \"NASDAQ\"),\n        (\"GOOGL\", \"NASDAQ\"),\n        (\"MSFT\", \"NASDAQ\"),\n        (\"AMZN\", \"NASDAQ\"),\n        (\"META\", \"NASDAQ\"),\n    ]\n    \n    print(f\"  Fetching {len(symbols)} symbols in parallel...\")\n    print(\"  Symbols:\", [s[0] for s in symbols])\n    print()\n    \n    start = time.time()\n    results = fetch_parallel(\n        fetcher,\n        symbols,\n        TimeFrame.DAILY,\n        bars=10,\n        max_workers=3,\n        show_progress=True\n    )\n    duration = time.time() - start\n    \n    print(f\"\\n  Total time: {duration:.2f}s\")\n    print(f\"  Average per symbol: {duration / len(symbols):.2f}s\")\n    print(f\"  Successful: {len(results)}/{len(symbols)}\")\n    \n    if results:\n        print(\"\\n  Sample data (first 2 rows each):\")\n        for key, df in list(results.items())[:3]:\n            print(f\"\\n  {key}:\")\n            print(df.head(2).to_string())\n    \n    return results\n\n\ndef demo_parallel_with_cache():\n    \"\"\"Demo: Parallel fetching with cache integration.\"\"\"\n    print_header(\"5. PARALLEL + CACHE DEMO\")\n    \n    fetcher = XnoxsFetcher()\n    cache = DataCache()\n    \n    symbols = [\n        (\"AAPL\", \"NASDAQ\"),\n        (\"TSLA\", \"NASDAQ\"),\n        (\"NVDA\", \"NASDAQ\"),\n    ]\n    \n    print(\"  First fetch (populates cache)...\")\n    parallel = ParallelFetcher(fetcher)\n    results1 = parallel.fetch_with_cache(symbols, TimeFrame.DAILY, bars=20, cache=cache)\n    print(f\"  Fetched: {len(results1)} symbols\")\n    \n    print(\"\\n  Second fetch (should use cache)...\")\n    results2 = parallel.fetch_with_cache(symbols, TimeFrame.DAILY, bars=20, cache=cache)\n    print(f\"  Fetched: {len(results2)} symbols\")\n    \n    print(f\"\\n  Cache stats: {cache.get_stats()}\")\n\n\ndef main():\n    \"\"\"Run all demos.\"\"\"\n    print_header(\"XnoxsFetcher v4.0 - New Features Demo\")\n    \n    print(\"  This demo showcases the new features:\")\n    print(\"  1. Auth Manager - Session persistence & token refresh\")\n    print(\"  2. Data Cache - Local SQLite caching\")\n    print(\"  3. Data Export - CSV, Excel, JSON export\")\n    print(\"  4. Parallel Fetch - Multi-symbol concurrent fetching\")\n    print(\"  5. Parallel + Cache - Combined for maximum efficiency\")\n    \n    demo_auth_manager()\n    \n    cache = demo_cache()\n    \n    fetcher = XnoxsFetcher()\n    data = fetcher.get_historical_data(\"AAPL\", \"NASDAQ\", TimeFrame.DAILY, 20)\n    demo_export(data)\n    \n    demo_parallel()\n    \n    demo_parallel_with_cache()\n    \n    print_header(\"Demo Complete!\")\n    print(\"  All new features demonstrated successfully.\")\n    print(\"  Check the 'exports' folder for exported files.\")\n    print(\"  Check '.tv_cache' folder for cached data.\")\n    \n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n","path":null,"size_bytes":6998,"size_tokens":null},"xnoxs_fetcher/parallel.py":{"content":"\"\"\"\nXnoxsFetcher Parallel Module\n\nThis module provides parallel/async data fetching capabilities\nfor retrieving data from multiple symbols simultaneously.\n\nAuthor: developerxnoxs\n\"\"\"\n\nfrom __future__ import annotations\n\nimport logging\nimport threading\nimport time\nfrom concurrent.futures import ThreadPoolExecutor, as_completed, Future\nfrom dataclasses import dataclass, field\nfrom typing import Optional, List, Dict, Any, Callable, Union, Tuple\n\nimport pandas as pd\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass FetchTask:\n    \"\"\"Represents a single fetch task.\"\"\"\n    symbol: str\n    exchange: str\n    timeframe: str\n    bars: int = 100\n    futures_contract: Optional[int] = None\n    extended_session: bool = False\n    \n    def __hash__(self):\n        return hash((self.symbol, self.exchange, self.timeframe, self.bars))\n    \n    def __eq__(self, other):\n        if not isinstance(other, FetchTask):\n            return False\n        return (\n            self.symbol == other.symbol and\n            self.exchange == other.exchange and\n            self.timeframe == other.timeframe and\n            self.bars == other.bars\n        )\n\n\n@dataclass\nclass FetchResult:\n    \"\"\"Result of a fetch operation.\"\"\"\n    task: FetchTask\n    data: Optional[pd.DataFrame]\n    success: bool\n    error: Optional[str] = None\n    duration_seconds: float = 0.0\n\n\n@dataclass\nclass ParallelConfig:\n    \"\"\"Configuration for parallel fetcher.\"\"\"\n    max_workers: int = 5\n    timeout_per_task: float = 60.0\n    retry_count: int = 2\n    retry_delay: float = 1.0\n    rate_limit_delay: float = 0.5\n\n\nclass ParallelFetcher:\n    \"\"\"\n    Parallel Data Fetcher for multiple symbols.\n    \n    Features:\n        - Concurrent data fetching with thread pool\n        - Progress tracking and callbacks\n        - Error handling and retry logic\n        - Rate limiting to avoid API blocks\n    \n    Example:\n        >>> from xnoxs_fetcher import XnoxsFetcher, TimeFrame\n        >>> fetcher = XnoxsFetcher()\n        >>> parallel = ParallelFetcher(fetcher)\n        >>> \n        >>> symbols = [\n        ...     (\"AAPL\", \"NASDAQ\"),\n        ...     (\"GOOGL\", \"NASDAQ\"),\n        ...     (\"MSFT\", \"NASDAQ\")\n        ... ]\n        >>> results = parallel.fetch_multiple(symbols, TimeFrame.DAILY, bars=100)\n    \n    Author: developerxnoxs\n    \"\"\"\n    \n    def __init__(\n        self,\n        fetcher: Any,\n        config: Optional[ParallelConfig] = None,\n        on_progress: Optional[Callable[[int, int, FetchResult], None]] = None,\n        on_complete: Optional[Callable[[List[FetchResult]], None]] = None\n    ):\n        \"\"\"\n        Initialize ParallelFetcher.\n        \n        Args:\n            fetcher: XnoxsFetcher instance\n            config: Parallel configuration\n            on_progress: Callback for progress updates (completed, total, result)\n            on_complete: Callback when all tasks complete\n        \"\"\"\n        self._fetcher = fetcher\n        self._config = config or ParallelConfig()\n        self._on_progress = on_progress\n        self._on_complete = on_complete\n        self._lock = threading.Lock()\n        self._completed_count = 0\n        self._results: List[FetchResult] = []\n    \n    def _fetch_single(self, task: FetchTask) -> FetchResult:\n        \"\"\"\n        Fetch data for a single task.\n        \n        Args:\n            task: Fetch task to execute\n            \n        Returns:\n            FetchResult with data or error\n        \"\"\"\n        start_time = time.time()\n        last_error = None\n        \n        for attempt in range(self._config.retry_count + 1):\n            try:\n                time.sleep(self._config.rate_limit_delay)\n                \n                from .core import TimeFrame\n                timeframe = TimeFrame.from_string(task.timeframe)\n                \n                data = self._fetcher.get_historical_data(\n                    symbol=task.symbol,\n                    exchange=task.exchange,\n                    timeframe=timeframe,\n                    bars=task.bars,\n                    futures_contract=task.futures_contract,\n                    extended_session=task.extended_session\n                )\n                \n                duration = time.time() - start_time\n                \n                if data is not None and not data.empty:\n                    return FetchResult(\n                        task=task,\n                        data=data,\n                        success=True,\n                        duration_seconds=duration\n                    )\n                else:\n                    last_error = \"No data returned\"\n                    \n            except Exception as e:\n                last_error = str(e)\n                logger.warning(\n                    f\"Fetch attempt {attempt + 1} failed for \"\n                    f\"{task.symbol}:{task.exchange}: {e}\"\n                )\n                \n                if attempt < self._config.retry_count:\n                    time.sleep(self._config.retry_delay * (attempt + 1))\n        \n        duration = time.time() - start_time\n        return FetchResult(\n            task=task,\n            data=None,\n            success=False,\n            error=last_error,\n            duration_seconds=duration\n        )\n    \n    def fetch_multiple(\n        self,\n        symbols: List[Tuple[str, str]],\n        timeframe: Any,\n        bars: int = 100,\n        futures_contract: Optional[int] = None,\n        extended_session: bool = False\n    ) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Fetch data for multiple symbols in parallel.\n        \n        Args:\n            symbols: List of (symbol, exchange) tuples\n            timeframe: Chart timeframe\n            bars: Number of bars per symbol\n            futures_contract: Futures contract number\n            extended_session: Include extended hours\n            \n        Returns:\n            Dictionary mapping \"SYMBOL:EXCHANGE\" to DataFrame\n        \"\"\"\n        timeframe_str = timeframe.value if hasattr(timeframe, 'value') else str(timeframe)\n        \n        tasks = [\n            FetchTask(\n                symbol=sym,\n                exchange=exch,\n                timeframe=timeframe_str,\n                bars=bars,\n                futures_contract=futures_contract,\n                extended_session=extended_session\n            )\n            for sym, exch in symbols\n        ]\n        \n        results = self.fetch_tasks(tasks)\n        \n        output = {}\n        for result in results:\n            if result.success and result.data is not None:\n                key = f\"{result.task.symbol}:{result.task.exchange}\"\n                output[key] = result.data\n        \n        return output\n    \n    def fetch_tasks(self, tasks: List[FetchTask]) -> List[FetchResult]:\n        \"\"\"\n        Execute multiple fetch tasks in parallel.\n        \n        Args:\n            tasks: List of FetchTask objects\n            \n        Returns:\n            List of FetchResult objects\n        \"\"\"\n        self._completed_count = 0\n        self._results = []\n        total_tasks = len(tasks)\n        \n        logger.info(f\"Starting parallel fetch of {total_tasks} symbols\")\n        \n        with ThreadPoolExecutor(max_workers=self._config.max_workers) as executor:\n            futures: Dict[Future, FetchTask] = {\n                executor.submit(self._fetch_single, task): task\n                for task in tasks\n            }\n            \n            for future in as_completed(futures, timeout=self._config.timeout_per_task * total_tasks):\n                task = futures[future]\n                \n                try:\n                    result = future.result(timeout=self._config.timeout_per_task)\n                except Exception as e:\n                    result = FetchResult(\n                        task=task,\n                        data=None,\n                        success=False,\n                        error=str(e)\n                    )\n                \n                with self._lock:\n                    self._completed_count += 1\n                    self._results.append(result)\n                    \n                    if self._on_progress:\n                        try:\n                            self._on_progress(\n                                self._completed_count, \n                                total_tasks, \n                                result\n                            )\n                        except Exception as e:\n                            logger.error(f\"Progress callback error: {e}\")\n                \n                status = \"OK\" if result.success else f\"FAILED: {result.error}\"\n                logger.info(\n                    f\"[{self._completed_count}/{total_tasks}] \"\n                    f\"{task.symbol}:{task.exchange} - {status}\"\n                )\n        \n        if self._on_complete:\n            try:\n                self._on_complete(self._results)\n            except Exception as e:\n                logger.error(f\"Complete callback error: {e}\")\n        \n        successful = sum(1 for r in self._results if r.success)\n        logger.info(\n            f\"Parallel fetch complete: {successful}/{total_tasks} successful\"\n        )\n        \n        return self._results\n    \n    def fetch_with_cache(\n        self,\n        symbols: List[Tuple[str, str]],\n        timeframe: Any,\n        bars: int = 100,\n        cache: Optional[Any] = None\n    ) -> Dict[str, pd.DataFrame]:\n        \"\"\"\n        Fetch data with cache support - only fetch missing data.\n        \n        Args:\n            symbols: List of (symbol, exchange) tuples\n            timeframe: Chart timeframe\n            bars: Number of bars\n            cache: DataCache instance\n            \n        Returns:\n            Dictionary mapping \"SYMBOL:EXCHANGE\" to DataFrame\n        \"\"\"\n        output = {}\n        to_fetch = []\n        timeframe_str = timeframe.value if hasattr(timeframe, 'value') else str(timeframe)\n        \n        for sym, exch in symbols:\n            key = f\"{sym}:{exch}\"\n            \n            if cache:\n                cached = cache.get(sym, exch, timeframe_str, bars)\n                if cached is not None:\n                    output[key] = cached\n                    logger.debug(f\"Cache hit: {key}\")\n                    continue\n            \n            to_fetch.append((sym, exch))\n        \n        if to_fetch:\n            logger.info(f\"Fetching {len(to_fetch)} symbols (from cache: {len(output)})\")\n            fetched = self.fetch_multiple(to_fetch, timeframe, bars)\n            \n            if cache:\n                for key, df in fetched.items():\n                    parts = key.split(\":\")\n                    if len(parts) == 2:\n                        cache.set(parts[0], parts[1], timeframe_str, bars, df)\n            \n            output.update(fetched)\n        \n        return output\n\n\nclass BatchExporter:\n    \"\"\"\n    Batch export for parallel fetch results.\n    \"\"\"\n    \n    @staticmethod\n    def results_to_combined_df(\n        results: Dict[str, pd.DataFrame]\n    ) -> pd.DataFrame:\n        \"\"\"\n        Combine multiple DataFrames into one.\n        \n        Args:\n            results: Dictionary of symbol -> DataFrame\n            \n        Returns:\n            Combined DataFrame with all data\n        \"\"\"\n        if not results:\n            return pd.DataFrame()\n        \n        dfs = []\n        for key, df in results.items():\n            df_copy = df.copy()\n            if 'symbol' not in df_copy.columns:\n                df_copy.insert(0, 'symbol', key)\n            dfs.append(df_copy)\n        \n        return pd.concat(dfs, ignore_index=False)\n    \n    @staticmethod\n    def results_summary(results: List[FetchResult]) -> Dict[str, Any]:\n        \"\"\"\n        Generate summary statistics for fetch results.\n        \n        Args:\n            results: List of FetchResult objects\n            \n        Returns:\n            Summary dictionary\n        \"\"\"\n        successful = [r for r in results if r.success]\n        failed = [r for r in results if not r.success]\n        \n        total_rows = sum(\n            len(r.data) for r in successful if r.data is not None\n        )\n        \n        avg_duration = (\n            sum(r.duration_seconds for r in results) / len(results)\n            if results else 0\n        )\n        \n        return {\n            \"total_tasks\": len(results),\n            \"successful\": len(successful),\n            \"failed\": len(failed),\n            \"success_rate\": len(successful) / len(results) * 100 if results else 0,\n            \"total_rows_fetched\": total_rows,\n            \"average_duration_seconds\": round(avg_duration, 2),\n            \"failed_symbols\": [\n                f\"{r.task.symbol}:{r.task.exchange}\" \n                for r in failed\n            ],\n            \"errors\": {\n                f\"{r.task.symbol}:{r.task.exchange}\": r.error \n                for r in failed if r.error\n            }\n        }\n\n\ndef fetch_parallel(\n    fetcher: Any,\n    symbols: List[Tuple[str, str]],\n    timeframe: Any,\n    bars: int = 100,\n    max_workers: int = 5,\n    show_progress: bool = True,\n    cache: Optional[Any] = None,\n    use_cache: bool = True,\n    max_age_minutes: Optional[int] = None,\n    force_refresh: bool = False,\n    progress_callback: Optional[Callable[[str, str, bool, int, int], None]] = None\n) -> Dict[Tuple[str, str], pd.DataFrame]:\n    \"\"\"\n    Quick parallel fetch function with cache support.\n    \n    Args:\n        fetcher: XnoxsFetcher instance\n        symbols: List of (symbol, exchange) tuples\n        timeframe: Chart timeframe\n        bars: Number of bars\n        max_workers: Number of parallel workers\n        show_progress: Print progress to console\n        cache: DataCache instance (optional)\n        use_cache: Whether to use cache (default True)\n        max_age_minutes: Maximum cache age in minutes. If cache is older, refresh from API.\n                        None means use default TTL (24 hours).\n        force_refresh: If True, always fetch fresh data from API (ignore cache)\n        progress_callback: Optional callback(symbol, exchange, success, current, total)\n        \n    Returns:\n        Dictionary mapping (symbol, exchange) tuple to DataFrame\n    \n    Example:\n        >>> # Basic usage - always fresh data\n        >>> results = fetch_parallel(\n        ...     fetcher,\n        ...     [(\"AAPL\", \"NASDAQ\"), (\"GOOGL\", \"NASDAQ\")],\n        ...     TimeFrame.DAILY,\n        ...     bars=100,\n        ...     force_refresh=True  # Always get latest data\n        ... )\n        \n        >>> # With cache - data max 30 minutes old\n        >>> results = fetch_parallel(\n        ...     fetcher,\n        ...     symbols,\n        ...     TimeFrame.DAILY,\n        ...     cache=cache,\n        ...     max_age_minutes=30  # Only use cache if < 30 minutes old\n        ... )\n    \"\"\"\n    timeframe_str = timeframe.value if hasattr(timeframe, 'value') else str(timeframe)\n    output: Dict[Tuple[str, str], pd.DataFrame] = {}\n    to_fetch: List[Tuple[str, str]] = []\n    \n    if cache and use_cache and not force_refresh:\n        for sym, exch in symbols:\n            should_use_cache = True\n            \n            if max_age_minutes is not None:\n                should_use_cache = cache.is_fresh(sym, exch, timeframe_str, bars, max_age_minutes)\n            \n            if should_use_cache:\n                cached = cache.get(sym, exch, timeframe_str, bars)\n                if cached is not None:\n                    output[(sym, exch)] = cached\n                    logger.debug(f\"Cache hit: {sym}:{exch}\")\n                    continue\n            \n            to_fetch.append((sym, exch))\n    else:\n        to_fetch = list(symbols)\n    \n    if to_fetch:\n        if show_progress and cache and use_cache and not force_refresh:\n            print(f\"  Using {len(output)} from cache, fetching {len(to_fetch)} from API...\")\n        \n        def internal_progress(completed, total, result):\n            if show_progress:\n                status = \"OK\" if result.success else \"FAILED\"\n                print(f\"  [{completed}/{total}] {result.task.symbol} - {status}\")\n            if progress_callback:\n                try:\n                    progress_callback(\n                        result.task.symbol,\n                        result.task.exchange,\n                        result.success,\n                        completed,\n                        total\n                    )\n                except Exception as e:\n                    logger.error(f\"Progress callback error: {e}\")\n        \n        config = ParallelConfig(max_workers=max_workers)\n        parallel = ParallelFetcher(\n            fetcher, \n            config=config,\n            on_progress=internal_progress if (show_progress or progress_callback) else None\n        )\n        \n        results = parallel.fetch_multiple(to_fetch, timeframe, bars)\n        \n        for key, df in results.items():\n            parts = key.split(\":\")\n            if len(parts) >= 2:\n                sym, exch = parts[0], parts[1]\n                output[(sym, exch)] = df\n                \n                if cache and df is not None and not df.empty:\n                    cache.set(sym, exch, timeframe_str, bars, df)\n    \n    return output\n","path":null,"size_bytes":17123,"size_tokens":null}},"version":2}